TITLE: Defining Data Models for Flight Booking System
DESCRIPTION: This snippet defines Pydantic models for the flight booking system, including FlightDetails, NoFlightFound, and Deps. It also creates agents for searching flights and extracting flight details.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
class FlightDetails(BaseModel):
    """Details of the most suitable flight."""

    flight_number: str
    price: int
    origin: str = Field(description='Three-letter airport code')
    destination: str = Field(description='Three-letter airport code')
    date: datetime.date


class NoFlightFound(BaseModel):
    """When no valid flight is found."""


@dataclass
class Deps:
    web_page_text: str
    req_origin: str
    req_destination: str
    req_date: datetime.date


# This agent is responsible for controlling the flow of the conversation.
# It accepts search parameters and returns either flight details or a "not found" response
search_agent = Agent[Deps, FlightDetails | NoFlightFound](
    model=llm,
    result_type=FlightDetails | NoFlightFound,  # type: ignore
    retries=4,
    system_prompt=(
        'Your job is to find the cheapest flight for the user on the given date. '
    ),
    instrument=True,
)

# This agent is responsible for extracting flight details from web page text.
# It parses the raw text and returns a structured list of flights
extraction_agent = Agent(
    model=llm,
    result_type=list[FlightDetails],
    system_prompt='Extract all the flight details from the given text.',
)
```

----------------------------------------

TITLE: Implementing Document Relevance Grading Function
DESCRIPTION: This function determines whether retrieved documents are relevant to the question. It uses a language model to grade the relevance and decides whether to generate an answer or rewrite the question based on the relevance score.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
from typing import Annotated, Literal, Sequence
from typing_extensions import TypedDict

from langchain import hub
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

from pydantic import BaseModel, Field


from langgraph.prebuilt import tools_condition

# This is the evaluation step
# It determines whether the retrieved documents are relevant to the question.
# If the documents are relevant, the agent will generate an answer.
# If the documents are not relevant, the agent will rewrite the question and try again.
def grade_documents(state) -> Literal["generate", "rewrite"]:
    """
    Determines whether the retrieved documents are relevant to the question.

    Args:
        state (messages): The current state

    Returns:
        str: A decision for whether the documents are relevant or not
    """

    print("---CHECK RELEVANCE---")

    # Data model
    class grade(BaseModel):
        """Binary score for relevance check."""

        binary_score: str = Field(description="Relevance score 'yes' or 'no'")

    # LLM
    model = ChatTogether(api_key=os.getenv("TOGETHER_API_KEY"), model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",temperature=0,streaming=True)

    # LLM with tool and validation
    llm_with_tool = model.with_structured_output(grade)

    # Prompt
    prompt = PromptTemplate(
        template="""You are a grader assessing relevance of a retrieved document to a user question. \n 
        Here is the retrieved document: \n\n {context} \n\n
        Here is the user question: {question} \n
        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \n
        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.""",
        input_variables=["context", "question"],
    )

    # Chain
    chain = prompt | llm_with_tool

    messages = state["messages"]
    last_message = messages[-1]

    question = messages[0].content
    docs = last_message.content

    scored_result = chain.invoke({"question": question, "context": docs})

    score = scored_result.binary_score

    if score == "yes":
        print("---DECISION: DOCS RELEVANT---")
        return "generate"

    else:
        print("---DECISION: DOCS NOT RELEVANT---")
        print(score)
        return "rewrite"
```

----------------------------------------

TITLE: Creating Retriever Tool for Agent
DESCRIPTION: This snippet creates a retriever tool that the agent will use to search for information in the blog posts. It wraps the retriever in a tool format that can be used by the LangChain agent.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
from langchain.tools.retriever import create_retriever_tool

# Create a tool for the retriever for the agent
retriever_tool = create_retriever_tool(
    retriever,
    "retrieve_blog_posts",
    "Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.",
)

tools = [retriever_tool]
```

----------------------------------------

TITLE: Implementing Query Generation Functions for Research Pipeline in Python
DESCRIPTION: Implements two functions for generating research queries: one to generate initial queries from a research topic using LLM, and another to wrap this with validation, limits, and logging. Uses the Together API for LLM interactions.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
async def generate_initial_queries(topic: str, together_client: AsyncTogether, max_queries: int, planning_model: str, json_model: str, prompts: dict) -> List[str]:
    """Step 1: Generate initial research queries based on the topic"""
    queries = await generate_research_queries(topic, together_client, planning_model, json_model, prompts)
    if max_queries > 0:
        queries = queries[:max_queries]
    print(f"\n\nInitial queries: {queries}")

    if len(queries) == 0:
        print("ERROR: No initial queries generated")
        return []

    return queries

async def generate_research_queries(topic: str, together_client: AsyncTogether, planning_model: str, json_model: str, prompts: dict) -> list[str]:
    """Generate research queries for a given topic using LLM"""
    PLANNING_PROMPT = prompts["planning_prompt"]

    planning_response = await together_client.chat.completions.create(
        model=planning_model,
        messages=[
            {"role": "system", "content": PLANNING_PROMPT},
            {"role": "user", "content": f"Research Topic: {topic}"}
        ]
    )
    plan = planning_response.choices[0].message.content

    print(f"Generated plan: {plan}")

    SEARCH_PROMPT = prompts["plan_parsing_prompt"]

    json_response = await together_client.chat.completions.create(
        model=json_model,
        messages=[
            {"role": "system", "content": SEARCH_PROMPT},
            {"role": "user", "content": f"Plan to be parsed: {plan}"}
        ],
        response_format={"type": "json_object", "schema": ResearchPlan.model_json_schema()}
    )

    response_json = json_response.choices[0].message.content
    plan = json.loads(response_json)
    return plan["queries"]
```

----------------------------------------

TITLE: Generating Response with DeepSeek R1 Reasoning Model
DESCRIPTION: Calls the DeepSeek R1 model to answer the query using the retrieved context, streaming the response as it's generated.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_16

LANGUAGE: python
CODE:
```
query = "What is the maximum allowable floating point operation per second this bill allows for model training?"


PROMPT = """
Answer the question: {query}. 
IMPORTANT RULE: Use the information provided to answer the question. For each claim in the answer provide a source from the information provided. 
Here is relevant information: {formatted_chunks} 
"""


stream = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1",
    messages=[
      {"role": "system", "content": "You are a helpful chatbot."},
      {"role": "user", "content": PROMPT.format(query=query, formatted_chunks=formatted_chunks)},
    ],
      stream=True,
)

response = ''

for chunk in stream:
  response += chunk.choices[0].delta.content or ""
  print(chunk.choices[0].delta.content or "", end="", flush=True)
```

----------------------------------------

TITLE: Creating Milvus Collection and Upserting Data
DESCRIPTION: This code snippet demonstrates how to create a Milvus collection with specified schema and index parameters, and then upsert data into the collection. It uses the `pymilvus` library to interact with Milvus. The code prepares data by generating embeddings for contextual chunks and then upserts the data along with document indices and titles into the Milvus collection.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
schema.add_field("document_index", DataType.VARCHAR, max_length=255)
        schema.add_field("embedding", DataType.FLOAT_VECTOR, dim=1024)
        schema.add_field("title", DataType.VARCHAR, max_length=255)
        index_params = client.prepare_index_params()
        index_params.add_index("embedding", metric_type="COSINE")

        client.create_collection(collection_name, dimension=512, schema=schema, index_params=index_params)

    if not document.contextual_chunks:
        return document  # Exit early if there are no contextual chunks
    
    # Generate embeddings for chunks
    embeddings = [get_embedding(chunk[:512], embedding_model) for chunk in document.contextual_chunks] # NOTE: Trimming the chunk for the embedding model's context window
    embeddings_np = np.array(embeddings, dtype=np.float32)

    ids = [
        f"id{document.idx}_{chunk_idx}"
        for chunk_idx, _ in enumerate(document.contextual_chunks)
    ]
    titles = [document.title] * len(document.contextual_chunks)

    client.upsert(
        collection_name,
        [
            {"id": index, "document_index": document_index, "embedding": embedding, "title": title}
            for index, (document_index, embedding, title) in enumerate(zip(ids, embeddings_np.tolist(), titles))
        ]
    )

    return document
```

----------------------------------------

TITLE: Loading and Preprocessing Documents for RAG System
DESCRIPTION: This code loads documents from web URLs, splits them into smaller chunks using a text splitter, and prepares them for embedding and storage in the vector database.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Load documents from web URLs and chunk them into smaller pieces
docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=100, chunk_overlap=50
)
doc_splits = text_splitter.split_documents(docs_list)
```

----------------------------------------

TITLE: Implementing Agent Tools and Validation for Flight Search
DESCRIPTION: This code defines tools and validation logic for the flight search agent. It includes a function to extract flights and a validator to ensure flight details meet the specified requirements.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
@search_agent.tool
async def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:
    """Get details of all flights."""
    # we pass the usage to the search agent so requests within this agent are counted
    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)
    logfire.info('found {flight_count} flights', flight_count=len(result.data))
    return result.data


@search_agent.result_validator
async def validate_result(
    ctx: RunContext[Deps], result: FlightDetails | NoFlightFound
) -> FlightDetails | NoFlightFound:
    """Procedural validation that the flight meets the constraints."""
    if isinstance(result, NoFlightFound):
        return result

    errors: list[str] = []
    if result.origin != ctx.deps.req_origin:
        errors.append(
            f'Flight should have origin {ctx.deps.req_origin}, not {result.origin}'
        )
    if result.destination != ctx.deps.req_destination:
        errors.append(
            f'Flight should have destination {ctx.deps.req_destination}, not {result.destination}'
        )
    if result.date != ctx.deps.req_date:
        errors.append(f'Flight should be on {ctx.deps.req_date}, not {result.date}')

    if errors:
        raise ModelRetry('\n'.join(errors))
    else:
        return result
```

----------------------------------------

TITLE: Creating a Simple Web Search Agent with Together AI
DESCRIPTION: Demonstrates how to create a basic agent with web search capability using Meta-Llama-3.1-8B-Instruct-Turbo model and DuckDuckGo search as a tool. The agent can answer questions by retrieving information from the web.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo", max_tokens=4096),
    tools=[DuckDuckGoTools()],
    markdown=True
)
agent.print_response("How are the golden state warriors doing this 2024-2025 season?", stream=True)
```

----------------------------------------

TITLE: Defining a Retrieval Task
DESCRIPTION: This code defines a Flyte task `retrieve` that performs retrieval using both a vector database (Milvus) and a BM25S keyword index. It generates embeddings for input queries, searches the Milvus collection, loads the BM25S index and contextual chunks data, performs BM25S-based retrieval, and returns the results as a dataclass. Requires `bm25s` and `pymilvus` libraries, as well as the pre-built indices.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
@dataclass
class RetrievalResults:
    vector_results: list[list[str]]
    bm25s_results: list[list[str]]


@union.task
def retrieve(
    bm25s_index: FlyteDirectory,
    contextual_chunks_data: FlyteFile,
    embedding_model: str = "BAAI/bge-large-en-v1.5",
    queries: list[str] = [
        "What to do in the face of uncertainty?",
        "Why won't people write?",
    ],
) -> RetrievalResults:
    import json

    import bm25s
    import numpy as np
    from pymilvus import MilvusClient

    client = MilvusClient("test_milvus.db")    
    
    # Generate embeddings for the queries using Together
    query_embeddings = [
        get_embedding(query, embedding_model) for query in queries
    ]
    query_embeddings_np = np.array(query_embeddings, dtype=np.float32)

    collection_name = "paul_graham_collection" 
    results = client.search(
        collection_name,
        query_embeddings_np,
        limit=5,
        search_params={"metric_type": "COSINE"},
        anns_field="embedding",
        output_fields=["document_index", "title"]
    )

    # Load BM25S index
    retriever = bm25s.BM25()
    bm25_index = retriever.load(save_dir=bm25s_index.download())

    # Load contextual chunk data
    with open(contextual_chunks_data, "r", encoding="utf-8") as json_file:
        contextual_chunks_data_dict = json.load(json_file)

    # Perform BM25S-based retrieval
    bm25s_idx_result = bm25_index.retrieve(
        query_tokens=bm25s.tokenize(queries),
        k=5,
        corpus=np.array(list(contextual_chunks_data_dict.values())),
    )

    # Return results as a dataclass
    return RetrievalResults(
        vector_results=results,
        bm25s_results=bm25s_idx_result.documents.tolist(),
    )
```

----------------------------------------

TITLE: Generating Response from LLM in Python
DESCRIPTION: This code snippet sends a request to a generative model with the query and relevant information to obtain a contextual answer to the user's question.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_42

LANGUAGE: python
CODE:
```
# Generate a story based on the top 10 most similar movies

query = "What are 'skip-level' meetings?"

response = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
    messages=[
      {"role": "system", "content": "You are a helpful chatbot."},
      {"role": "user", "content": f"Answer the question: {query}. Here is relevant information: {retreived_chunks}"},
    ],
)
```

----------------------------------------

TITLE: Generating Contextual Prompts for Each Chunk
DESCRIPTION: Defines a function that generates prompts for each chunk by filling in the prompt template with the full document and individual chunk content. Returns a list of complete prompts.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
from typing import List

# First we will just generate the prompts and examine them

def generate_prompts(document : str, chunks : List[str]) -> List[str]:
  prompts = []
  for chunk in chunks:
    prompt = CONTEXTUAL_RAG_PROMPT.format(WHOLE_DOCUMENT=document, CHUNK_CONTENT=chunk)
    prompts.append(prompt)
  return prompts
```

----------------------------------------

TITLE: Looping Workflow Implementation
DESCRIPTION: Implementation of the main workflow that coordinates between Generator and Evaluator in a loop until acceptance.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
def loop_workflow(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:
    """Keep generating and evaluating until the evaluator passes the last generated response."""
    # Store previous responses from generator
    memory = []
    
    # Generate initial response
    response = generate(task, generator_prompt)
    memory.append(response)

    #Build a schema for the evaluation
    class Evaluation(BaseModel):
        evaluation: Literal["PASS", "NEEDS_IMPROVEMENT", "FAIL"]
        feedback: str

    # While the generated response is not passing, keep generating and evaluating
    while True:
        evaluation, feedback = evaluate(task, evaluator_prompt, response, Evaluation)
        # Terminating condition
        if evaluation == "PASS":
            return response
        
        # Add current response and feedback to context and generate a new response
        context = "\n".join([
            "Previous attempts:",
            *[f"- {m}" for m in memory],
            f"\nFeedback: {feedback}"
        ])
        
        response = generate(generator_prompt, task, context)
        memory.append(response)
```

----------------------------------------

TITLE: Querying Llama 3.2 90B Vision with Retrieved Image
DESCRIPTION: Uses the Together AI client to query the Llama 3.2 90B Vision model with the retrieved image and the original question, demonstrating multimodal question answering capabilities.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
import os
from together import Together

client = Together(api_key = api_key)

response = client.chat.completions.create(
  model="meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo",
  messages=[
    {
      "role": "user",
      "content": [
        {"type": "text", "text": query}, #query
        {
          "type": "image_url",
          "image_url": {
            "url": f"data:image/jpeg;base64,{returned_page}", #retrieved page image
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0].message.content)
```

----------------------------------------

TITLE: Executing a Complex Query with the Planning Agent
DESCRIPTION: Demonstrates how the agent handles a real query by breaking it down, executing steps, and adapting the plan as needed. It includes error handling for potential issues during execution.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
config = {"recursion_limit": 25}
inputs = {"input": "what is the hometown of the mens 2024 Australia open winner?"}
try:
    async for event in app.astream(inputs, config=config):
        for k, v in event.items():
            if k != "__end__":
                print(v)
except IndexError as e:
    print(f"Error: {e}. The plan list is empty or index is out of range.")
    print("Execution terminated.")
except Exception as e:
    print(f"An error occurred: {e}")
    print("Execution terminated.")
```

----------------------------------------

TITLE: Defining Core Planning Functions
DESCRIPTION: Implements three main functions that drive the planning process: execute_step, plan_step, and replan_step. These functions handle execution, initial planning, and plan revision.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
from typing import Union


class Response(BaseModel):
    """Response to user."""

    response: str


class Act(BaseModel):
    """Action to perform."""

    action: Union[Response, Plan] = Field(
        description="Action to perform. If you want to respond to user, use Response. "
        "If you need to further use tools to get the answer, use Plan."
    )


replanner_prompt = ChatPromptTemplate.from_template(
    """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.

Your objective was this:
{input}

Your original plan was this:
{plan}

You have currently done the follow steps:
{past_steps}

Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan."""
)


replanner = replanner_prompt | ChatTogether(model="meta-llama/Llama-3.3-70B-Instruct-Turbo", temperature=0).with_structured_output(Act)
```

LANGUAGE: python
CODE:
```
from typing import Literal
from langgraph.graph import END


async def execute_step(state: PlanExecute):
    plan = state["plan"]
    plan_str = "\n".join(f"{i+1}. {step}" for i, step in enumerate(plan))
    task = plan[0]
    task_formatted = f"""For the following plan:
{plan_str}\n\nYou are tasked with executing step {1}, {task}."""
    agent_response = await agent_executor.ainvoke(
        {"messages": [("user", task_formatted)]}
    )
    return {
        "past_steps": [(task, agent_response["messages"][-1].content)],
    }


async def plan_step(state: PlanExecute):
    plan = await planner.ainvoke({"messages": [("user", state["input"])]})
    return {"plan": plan.steps}


async def replan_step(state: PlanExecute):
    output = await replanner.ainvoke(state)
    if isinstance(output.action, Response):
        return {"response": output.action.response}
    else:
        return {"plan": output.action.steps}


def should_end(state: PlanExecute):
    if "response" in state and state["response"]:
        return END
    else:
        return "agent"
```

----------------------------------------

TITLE: Implementing Vector Retrieval Function
DESCRIPTION: Creates a compact function that retrieves the top-k most similar chunks from a vector index based on a query embedding.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
# compact function to retrieve the top k chunks

def vector_retrieval(query: str, top_k: int = 5, vector_index: np.ndarray = None, chunks: List[str] = None) -> List[int]:
    """
    Retrieve the top-k most similar items from an index based on a query.
    Args:
        query (str): The query string to search for.
        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.
        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.
    Returns:
        List[int]: A list of indices corresponding to the top-k most similar items in the index.
    """

    query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]
    
    
    dot_product = np.dot(query_embedding, np.array(vector_index).T)
    query_norm = np.linalg.norm(query_embedding)
    vector_index_norm = np.linalg.norm(vector_index, axis=1)
    
    similarity_scores = dot_product / (query_norm * vector_index_norm)

    return [chunks[index] for index in np.argsort(-similarity_scores)[:top_k]]
```

----------------------------------------

TITLE: Creating a Generic Parallel Workflow Function
DESCRIPTION: Implements a reusable function that orchestrates the entire parallel workflow pattern, from receiving parallel proposals to aggregating them into a final response.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
async def parallel_workflow(prompt : str, proposer_models : List[str], aggregator_model : str, aggregator_prompt: str):
    """Run a parallel chain of LLM calls to address the `input_query` 
    using a list of models specified in `models`.

    Returns output from final aggregator model.
    """

    # Gather intermediate responses from proposer models
    proposed_responses = await asyncio.gather(*[run_llm_parallel(prompt, model) for model in proposer_models])
    
    # Aggregate responses using an aggregator model
    final_output = run_llm(user_prompt=prompt,
                           model=aggregator_model,
                           system_prompt=aggregator_prompt + "\n" + "\n".join(f"{i+1}. {str(element)}" for i, element in enumerate(proposed_responses)
           ))
    
    return final_output, proposed_responses
```

----------------------------------------

TITLE: Importing Libraries and Defining Knowledge Base URLs
DESCRIPTION: This snippet imports the required libraries for document loading, vector storage, and language models. It also defines a list of blog post URLs that will serve as the knowledge base for the RAG system.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
# Import required libraries for document loading, vector storage, and language models
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_together import ChatTogether

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_together import TogetherEmbeddings

# List of blog posts we'll use as our knowledge base
urls = [
    "https://lilianweng.github.io/posts/2023-06-23-agent/",
    "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/",
    "https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/",
]
```

----------------------------------------

TITLE: Aggregating Intermediate Responses with Final LLM
DESCRIPTION: Uses an advanced language model to analyze and synthesize the collected intermediate responses into a final, comprehensive answer for the user.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
# We will use the best open source model to aggregate the responses
aggregator_model = "deepseek-ai/DeepSeek-V3"

final_output = run_llm(user_prompt=user_prompt, # task to be completed
                       model=aggregator_model,
                       system_prompt=aggregator_system_prompt + "\n" + "\n".join(f"{i+1}. {str(element)}" for i, element in enumerate(results)
           ))

print(final_output)
```

----------------------------------------

TITLE: Implementing Embedding Generation Function
DESCRIPTION: Defines a function to generate embeddings using the Together AI API. The function takes a list of texts and a model identifier, then returns embeddings for each text using the specified model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# This function will be used to access the Together API to generate embeddings for the movie plots

from typing import List

def generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:
    """Generate embeddings from Together python library.

    Args:
        input_texts: a list of string input texts.
        model_api_string: str. An API string for a specific embedding model of your choice.

    Returns:
        embeddings_list: a list of embeddings. Each element corresponds to the each input text.
    """
    together_client = together.Together(api_key = TOGETHER_API_KEY)
    outputs = together_client.embeddings.create(
        input=input_texts,
        model=model_api_string,
    )
    return [x.embedding for x in outputs.data]
```

----------------------------------------

TITLE: Defining Agentic Workflow with LangGraph
DESCRIPTION: This code snippet defines a workflow for an agent using the LangGraph library. It initializes a StateGraph, adds nodes representing different actions like agent, retrieval, rewriting, and response generation. It defines edges between nodes, including conditional edges based on agent decisions and document grading. Finally, it compiles the graph to create the workflow.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
from langgraph.graph import END, StateGraph, START
from langgraph.prebuilt import ToolNode

# Define a new graph
workflow = StateGraph(AgentState)

# Define the nodes we will cycle between
workflow.add_node("agent", agent)  # agent
retrieve = ToolNode([retriever_tool])
workflow.add_node("retrieve", retrieve)  # retrieval
workflow.add_node("rewrite", rewrite)  # Re-writing the question
workflow.add_node(
    "generate", generate
)  # Generating a response after we know the documents are relevant

# Call agent node to decide to retrieve or not
workflow.add_edge(START, "agent")

# Decide whether to retrieve
workflow.add_conditional_edges(
    "agent",
    # Assess agent decision
    tools_condition,
    {
        # Translate the condition outputs to nodes in our graph
        "tools": "retrieve",
        END: END,
    },
)

# Edges taken after the `action` node is called.
workflow.add_conditional_edges(
    "retrieve",
    # Assess agent decision
    grade_documents,
)
workflow.add_edge("generate", END)
workflow.add_edge("rewrite", "agent")

# Compile
graph = workflow.compile()
```

----------------------------------------

TITLE: Creating BM25S Index
DESCRIPTION: This code snippet demonstrates how to create a BM25S keyword index using the `bm25s` library.  It prepares data from documents, creates a BM25 index, saves the index to a directory, and writes contextual chunks to a JSON file. The function is decorated with `@actor.task` indicating it's part of a distributed compute environment.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
@actor.task(cache=True, cache_version="0.5")
def create_bm25s_index(documents: list[Document]) -> tuple[FlyteDirectory, FlyteFile]:
    import json
    import bm25s

    # Prepare data for JSON
    data = {
        f"id{doc_idx}_{chunk_idx}": contextual_chunk
        for doc_idx, document in enumerate(documents)
        if document.contextual_chunks
        for chunk_idx, contextual_chunk in enumerate(document.contextual_chunks)
    }

    retriever = bm25s.BM25(corpus=list(data.values()))
    retriever.index(bm25s.tokenize(list(data.values())))

    ctx = union.current_context()
    working_dir = Path(ctx.working_directory)
    bm25s_index_dir = working_dir / "bm25s_index"
    contextual_chunks_json = working_dir / "contextual_chunks.json"

    retriever.save(str(bm25s_index_dir))

    # Write the data to a JSON file
    with open(contextual_chunks_json, "w", encoding="utf-8") as json_file:
        json.dump(data, json_file, indent=4, ensure_ascii=False)

    return FlyteDirectory(path=bm25s_index_dir), FlyteFile(contextual_chunks_json)
```

----------------------------------------

TITLE: Implementing Asynchronous Tavily Search and Content Summarization in Python
DESCRIPTION: This function performs a single search using the Tavily API and summarizes the raw content of each result using an LLM. It retrieves search results, filters out entries with no raw content, and processes each valid result asynchronously.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
async def tavily_search(query: str, tavily_client: AsyncTavilyClient, prompts: dict, together_client: AsyncTogether, summary_model: str) -> SearchResults:
    """Perform a single Tavily search"""
    print(f'Perform Tavily search with query: {query}')

    response = await tavily_client.search(query, include_raw_content=True)

    print(f"Tavily Responded with {len(response['results'])} results (Tavily returning None will be ignored for summarization)")

    RAW_CONTENT_SUMMARIZER_PROMPT = prompts["raw_content_summarizer_prompt"]

    # Create tasks for summarization
    summarization_tasks = []
    result_info = []
    for result in response["results"]:
        if result["raw_content"] is None:
            continue
        task = summarize_content(result["raw_content"], query, RAW_CONTENT_SUMMARIZER_PROMPT, together_client, summary_model)
        summarization_tasks.append(task)
        result_info.append(result)

    summarized_contents = await asyncio.gather(*summarization_tasks)

    formatted_results = []
    for result, summarized_content in zip(result_info, summarized_contents):
        formatted_results.append(
            SearchResult(
                title=result["title"],
                link=result["url"],
                content=result["raw_content"],
                filtered_raw_content=summarized_content,
            )
        )
    return SearchResults(formatted_results)

async def summarize_content(raw_content: str, query: str, prompt: str, together_client: AsyncTogether, summary_model: str) -> str:
    """Summarize content asynchronously using the LLM"""
    print("Summarizing content asynchronously using the LLM")

    summarize_response = await together_client.chat.completions.create(
        model=summary_model,
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"<Raw Content>{raw_content}</Raw Content>\n\n<Research Topic>{query}</Research Topic>"}
        ]
    )
    return summarize_response.choices[0].message.content
```

----------------------------------------

TITLE: Querying BM25 Index for Top-K Results in Python
DESCRIPTION: This snippet queries the BM25 index with a specified search query and retrieves the top k results, including their scores. It showcases the practical application of the BM25 search method.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_30

LANGUAGE: python
CODE:
```
# Query the corpus and get top-k results
query = "What are 'skip-level' meetings?"
results, scores = retriever.retrieve(bm25s.tokenize(query), k=5,)
```

----------------------------------------

TITLE: Setting up Tavily Search Tool
DESCRIPTION: Configures the Tavily search tool to allow the agent to find information online. It requires a Tavily API key from the environment variables.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
from langchain_community.tools.tavily_search import TavilySearchResults

tools = [TavilySearchResults(max_results=3, tavily_api_key=os.environ.get("TAVILY_API_KEY"))]
```

----------------------------------------

TITLE: Implementing Asynchronous LLM Function with Retry Logic
DESCRIPTION: Defines an asynchronous function for parallel LLM calls with built-in retry logic for handling rate limit errors from the Together API.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# The function below will call the reference LLMs in parallel
async def run_llm_parallel(user_prompt : str, model : str, system_prompt : str = None):
    """Run parallel LLM call with a reference model."""
    for sleep_time in [1, 2, 4]:
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
    
            messages.append({"role": "user", "content": user_prompt})

            response = await async_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=2000,
            )
            break
        except together.error.RateLimitError as e:
            print(e)
            await asyncio.sleep(sleep_time)
    return response.choices[0].message.content
```

----------------------------------------

TITLE: Calculating Cosine Similarity for Vector Retrieval
DESCRIPTION: Computes cosine similarity between the query embedding and document embeddings to find the most relevant chunks.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
# Calculate cosine similarity between the query embedding and each movie embedding

dot_product = np.dot(query_embedding, np.array(embeddings).T)
query_norm = np.linalg.norm(query_embedding)
embeddings_norm = np.linalg.norm(embeddings, axis=1)
similarity_scores = dot_product / (query_norm * embeddings_norm)
indices = np.argsort(-similarity_scores)
```

----------------------------------------

TITLE: Handling Seat Selection with Natural Language Processing in Python
DESCRIPTION: Function that processes user seat preferences using natural language. It repeatedly asks for seat preferences until it can parse the request into a valid SeatPreference object, maintaining conversation context through message history.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
async def find_seat(usage: Usage) -> SeatPreference:
    """Function to handle seat selection through natural language."""
    message_history: list[ModelMessage] | None = None
    while True:
        answer = Prompt.ask('What seat would you like?')

        result = await seat_preference_agent.run(
            answer,
            message_history=message_history,
            usage=usage,
            usage_limits=usage_limits,
        )
        if isinstance(result.data, SeatPreference):
            return result.data
        else:
            print('Could not understand seat preference. Please try again.')
            message_history = result.all_messages()
```

----------------------------------------

TITLE: Querying Indexed Document with ColQwen2
DESCRIPTION: Demonstrates how to query the indexed document using ColQwen2, retrieving the most relevant pages based on a given query about Nvidia's data center revenue.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
query = "What are the half year data centre renevue results and the 5 year CAGR for Nvidia data centre revenue?"
results = model.search(query, k=5)

print(f"Search results for '{query}':")
for result in results:
    print(f"Doc ID: {result.doc_id}, Page: {result.page_num}, Score: {result.score}")

print("Test completed successfully!")
```

----------------------------------------

TITLE: Defining System Prompts for Research Pipeline in Python
DESCRIPTION: Defines a dictionary of prompt templates for each stage of the research process, including planning, parsing, content processing, evaluation, filtering, and final answer generation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
# System Prompts
# -------------
# Instructions for each stage of the research process
prompts = {
    # Planning: Generates initial research queries
    "planning_prompt": """You are a strategic research planner with expertise in breaking down complex
                         questions into logical search steps. Generate focused, specific, and self-contained queries that
                         will yield relevant information for the research topic.""",

    # Plan Parsing: Extracts structured data from planning output
    "plan_parsing_prompt": """Extract search queries that should be executed.""",

    # Content Processing: Identifies relevant information from search results
    "raw_content_summarizer_prompt": """Extract and synthesize only the information relevant to the research
                                       topic from this content. Preserve specific data, terminology, and
                                       context while removing irrelevant information.""",

    # Completeness Evaluation: Determines if more research is needed
    "evaluation_prompt": """Analyze these search results against the original research goal. Identify
                          specific information gaps and generate targeted follow-up queries to fill
                          those gaps. If no significant gaps exist, indicate that research is complete.""",

    # Evaluation Parsing: Extracts structured data from evaluation output
    "evaluation_parsing_prompt": """Extract follow-up search queries from the evaluation. If no follow-up queries are needed, return an empty list.""",

    # Source Filtering: Selects most relevant sources
    "filter_prompt": """Evaluate each search result for relevance, accuracy, and information value
                       related to the research topic. At the end, you need to provide a list of
                       source numbers with the rank of relevance. Remove the irrelevant ones.""",
    # Source Filtering: Selects most relevant sources
    "source_parsing_prompt": """Extract the source list that should be included.""",

    # Answer Generation: Creates final research report
    "answer_prompt": """Create a comprehensive, publication-quality markdown research report based exclusively
                       on the provided sources. The report should include: title, introduction, analysis (multiple sections with insights titles)
                       and conclusions, references. Use proper citations (source with link; using \n\n \\[Ref. No.\\] to improve format),
                       organize information logically, and synthesize insights across sources. Include all relevant details while
                       maintaining readability and coherence. In each section, You MUST write in plain
                       paragraghs and NEVER describe the content following bullet points or key points (1,2,3,4... or point X: ...)
                       to improve the report readability."""
}
```

----------------------------------------

TITLE: Setting Up Vector Database for Semantic Search
DESCRIPTION: This snippet creates a vector database using Chroma and the processed documents. It sets up a retriever for semantic searches, which will be used by the agent to find relevant information when answering questions.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
import os

# Add to vectorDB
vectorstore = Chroma.from_documents(
    documents=doc_splits,
    collection_name="rag-chroma",
    embedding=TogetherEmbeddings(model="togethercomputer/m2-bert-80M-2k-retrieval", # long context embedding model
                                 api_key=os.getenv("TOGETHER_API_KEY")),
)
retriever = vectorstore.as_retriever()
```

----------------------------------------

TITLE: Retrieving Final Documents from Hybrid Search in Python
DESCRIPTION: This snippet constructs a list of documents based on the indices provided by the hybrid result from the RRF algorithm, aiming to compile the final relevant contextual information.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_37

LANGUAGE: python
CODE:
```
hybrid_top_k_docs = [contextual_chunks[index] for index in hybrid_top_k[1]]
```

----------------------------------------

TITLE: Creating a DPO Fine-tuning Job Continuing from SFT Checkpoint
DESCRIPTION: Configures a Direct Preference Optimization (DPO) fine-tuning job that continues from a previous SFT checkpoint. This implements the second stage of the two-stage approach, using the from_checkpoint parameter to continue training from the SFT results.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
dpo_training_from_sft = client.fine_tuning.create(
    training_file=dpo_train_file.id,
    validation_file=dpo_validation_file.id,
    n_evals=10,
    #model=MODEL_NAME, We do not use model name here, it is derived from the checkpoint!
    wandb_api_key=WANDB_API_KEY,
    wandb_project_name="helpsteer2",
    suffix="helpsteer2_dpo_training_continuing_sft",
    n_epochs=1,
    n_checkpoints=1,
    learning_rate=1e-5,
    lora=True,
    training_method='dpo', # Now we use DPO training
    from_checkpoint=sft_training.id # Continuing from SFT checkpoint!
)
print(dpo_training_from_sft.id)
```

----------------------------------------

TITLE: Synthesizing Final Answer from Worker Responses
DESCRIPTION: Uses another LLM call to synthesize the worker responses into a coherent final answer that addresses all aspects of the original task.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
SYNTHESIZER_PROMPT = """Given the task: {task} and the following responses below, that each address different aspects of the 
task, synthesize a final response.

{worker_responses}
"""
concatenated_responses = " ".join([f"\n=== WORKER RESULT ({task_info['type']}) ===\n{response}\n" for task_info, response in zip(tasks, worker_resp)])

final_answer = run_llm(SYNTHESIZER_PROMPT.format(task=task, worker_responses=concatenated_responses), model="meta-llama/Llama-3.3-70B-Instruct-Turbo")
```

----------------------------------------

TITLE: Using Reranker for Result Quality Improvement in Python
DESCRIPTION: This code snippet passes the final documents obtained from the hybrid search into a reranker model for enhanced relevance based on the original query.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_39

LANGUAGE: python
CODE:
```
from together import Together

query = "What are 'skip-level' meetings?" # we keep the same query - can change if we want

response = client.rerank.create(
  model="Salesforce/Llama-Rank-V1",
  query=query,
  documents=hybrid_top_k_docs,
  top_n=3 # we only want the top 3 results
)
```

----------------------------------------

TITLE: Implementing Conditional Image Generation Using FLUX Models in Python
DESCRIPTION: Defines a function to generate images conditioned on input images using the FLUX diffusion model from Together API.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
from together import Together

client = Together(api_key = 'TOGETHER_API_KEY')

def generate_image(image_prompt, retrieved_image, model = "black-forest-labs/FLUX.1-depth"):

    imageCompletion = client.images.generate(
        model = model,
        width=1024,
        height=768,
        steps=28,
        prompt = image_prompt,
        image_url = retrieved_image,
    )

    return imageCompletion.data[0].url
```

----------------------------------------

TITLE: Defining Agent State for LangGraph
DESCRIPTION: This code defines the AgentState class, which represents the state of the agent in the LangGraph framework. It uses TypedDict to specify the structure of the state, including a sequence of messages.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
from typing import Annotated, Sequence
from typing_extensions import TypedDict

from langchain_core.messages import BaseMessage

from langgraph.graph.message import add_messages


class AgentState(TypedDict):
    # The add_messages function defines how an update should be processed
    # Default is to replace. add_messages says "append"
    messages: Annotated[Sequence[BaseMessage], add_messages]
```

----------------------------------------

TITLE: Router Workflow Implementation - Task Execution
DESCRIPTION: Implements the complete router workflow function that selects and executes appropriate models for given tasks
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
def run_router_workflow(user_prompt : str):
    selected_model = JSON_llm(ROUTER_PROMPT.format(user_query=user_prompt),
                            ModelOutput,
                            system_prompt=ROUTER_SYSTEM_PROMPT)
    
    response = run_llm(user_prompt= user_prompt, 
                   model = selected_model['model']
    )
    return selected_model['model'], selected_model['reason'], response
```

----------------------------------------

TITLE: Building the Workflow Graph
DESCRIPTION: Creates a directed graph that coordinates the planning and execution process. It defines the flow between planning, execution, and replanning steps.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
from langgraph.graph import StateGraph, START

workflow = StateGraph(PlanExecute)

# Add the plan node
workflow.add_node("planner", plan_step)

# Add the execution step
workflow.add_node("agent", execute_step)

# Add a replan node
workflow.add_node("replan", replan_step)

workflow.add_edge(START, "planner")

# From plan we go to agent
workflow.add_edge("planner", "agent")

# From agent, we replan
workflow.add_edge("agent", "replan")

workflow.add_conditional_edges(
    "replan",
    # Next, we pass in the function that will determine which node is called next.
    should_end,
    ["agent", END],
)

# Finally, we compile it!
# This compiles it into a LangChain Runnable,
# meaning you can use it as you would any other runnable
app = workflow.compile()
```

----------------------------------------

TITLE: Creating Pydantic Models for Invoice Structure
DESCRIPTION: Defines Pydantic models to structure the data extraction. The Item class represents invoice line items with name, price, and quantity, while the Receipt class represents the complete invoice with items and total.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
import json
from pydantic import BaseModel, Field

class Item(BaseModel):
    name: str
    price: float
    quantity: int = Field(default=1)

class Receipt(BaseModel):
    items: list[Item]
    total: float
```

----------------------------------------

TITLE: Filtering and Ranking Search Results with Together AI
DESCRIPTION: Implements search result filtering and ranking based on relevance using Together AI's LLM. The function evaluates and prioritizes sources while filtering out less relevant content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_16

LANGUAGE: python
CODE:
```
async def filter_results(topic: str, results: SearchResults, together_client: AsyncTogether, json_model: str, max_sources: int, prompts: dict) -> tuple[SearchResults, SourceList]:
    """
    Filter and rank search results based on relevance to the research topic.

    Args:
        topic: The research topic
        results: Search results to filter
        together_client: The Together AI client for LLM operations
        json_model: Model to use for filtering
        max_sources: Maximum number of sources to keep (-1 for unlimited)
        prompts: Dictionary of prompt templates

    Returns:
        Tuple of (filtered results, source list with indices)
    """
    # Format the search results for the LLM, without the raw content
    formatted_results = results.short_str()

    FILTER_PROMPT = prompts["filter_prompt"]

    SOURCE_PARSING_PROMPT = prompts['source_parsing_prompt']

    llm_filter_response = await together_client.chat.completions.create(
        model=json_model,
        messages=[
            {"role": "system", "content": FILTER_PROMPT},
            {"role": "user", "content": (
                f"<Research Topic>{topic}</Research Topic>\n\n"
                f"<Current Search Results>{formatted_results}</Current Search Results>"
            )}
        ]
    )

    llm_filter_response_content = llm_filter_response.choices[0].message.content
    print(f'filter response: {llm_filter_response_content}')

    json_response = await together_client.chat.completions.create(
        model=json_model,
        messages=[
            {"role": "system", "content": SOURCE_PARSING_PROMPT},
            {"role": "user", "content": f"<FILTER_RESPONSE>{llm_filter_response_content}</FILTER_RESPONSE>"}
        ],
        response_format={"type": "json_object", "schema": SourceList.model_json_schema()}
    )

    response_json = json_response.choices[0].message.content
    evaluation = json.loads(response_json)
    sources = evaluation["sources"]

    print(f'sources ranked by relevance {sources} (we will keep maximum of {max_sources} sources, as defined by the user)')

    if max_sources > 0:
        sources = sources[:max_sources]

    # Filter the results based on the source list
    filtered_results = [results.results[i] for i in sources if i < len(results.results)]

    return SearchResults(filtered_results), sources
```

----------------------------------------

TITLE: Installing Required Libraries for Agentic RAG System
DESCRIPTION: This code snippet installs the necessary Python libraries for building the agentic RAG system, including LangChain, Together AI, Chroma, LangGraph, and other dependencies.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_0

LANGUAGE: python
CODE:
```
%%capture --no-stderr
%pip install -qU langchain-community tiktoken langchain-together together langchainhub chromadb langchain langgraph langchain-text-splitters beautifulsoup4
```

----------------------------------------

TITLE: Deploying FastAPI and Gradio Applications with Union in Python
DESCRIPTION: This Python snippet defines the deployment specifications for FastAPI and Gradio applications using the Union framework. It creates two applications, one for each framework, specifying input sources, container images, resource limits, and network settings. Required packages and environment variables are configured within the deployment specification. The snippet includes details on how to initialize these apps, set up dependencies, and the use of secrets for secure access. The FastAPI app is configured to use 'uvicorn' for serving, while the Gradio app runs a Python script. These applications form part of an experimental setup on the Union platform.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
import os

from union.app import App, Input


fastapi_app = App(
    name="contextual-rag-fastapi",
    inputs=[
        Input(
            name="bm25s_index",
            value=BM25Index.query(),
            download=True,
            env_var="BM25S_INDEX",
        ),
        Input(
            name="contextual_chunks_json",
            value=ContextualChunksJSON.query(),
            download=True,
            env_var="CONTEXTUAL_CHUNKS_JSON",
        ),
    ],
    container_image=union.ImageSpec(
        name="contextual-rag-fastapi",
        packages=[
            "together",
            "bm25s",
            "pymilvus",
            "uvicorn[standard]",
            "fastapi[standard]",
            "union-runtime>=0.1.10",
            "flytekit>=1.15.0b5",
        ],
    ),
    limits=union.Resources(cpu="1", mem="3Gi"),
    port=8080,
    include=["fastapi_app.py"],
    args=["uvicorn", "fastapi_app:app", "--port", "8080"],
    min_replicas=1,
    max_replicas=1,
    secrets=[
        fl.Secret(
            key="together-api-key", 
            env_var="TOGETHER_API_KEY", 
            mount_requirement=union.Secret.MountType.ENV_VAR
        ),
        fl.Secret(
            key="milvus-uri",
            env_var="MILVUS_URI",
            mount_requirement=union.Secret.MountType.ENV_VAR,
        ),
        fl.Secret(
            key="milvus-token",
            env_var="MILVUS_TOKEN",
            mount_requirement=union.Secret.MountType.ENV_VAR,
        ),
    ],
)


gradio_app = App(
    name="contextual-rag-gradio",
    inputs=[
        Input(
            name="fastapi_endpoint",
            value=fastapi_app.query_endpoint(public=False),
            env_var="FASTAPI_ENDPOINT",
        )
    ],
    container_image=union.ImageSpec(
        name="contextual-rag-gradio",
        packages=["gradio", "union-runtime>=0.1.5"],
    ),
    limits=union.Resources(cpu="1", mem="1Gi"),
    port=8080,
    include=["gradio_app.py"],
    args=[
        "python",
        "gradio_app.py",
    ],
    min_replicas=1,
    max_replicas=1,
)
```

----------------------------------------

TITLE: Indexing PDF with ColQwen2 for MultiModal RAG
DESCRIPTION: Uses ColQwen2 to index the Nvidia presentation PDF, creating embeddings for page images and storing them along with base64 encoded images for retrieval.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
index_name = "nvidia_index"
model.index(input_path=Path("/content/nvidia_presentation.pdf"),
    index_name=index_name,
    store_collection_with_index=True, # Stores base64 images along with the vectors
    overwrite=True
)
```

----------------------------------------

TITLE: Implementing Asynchronous Flight Booking Workflow in Python
DESCRIPTION: The main function that orchestrates the flight booking process. It configures search parameters, searches for flights, presents results to the user, and handles the booking flow. The loop continues until the user decides to book a flight or no flights are found.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
async def main():
    # Configure our search parameters
    deps = Deps(
        web_page_text=flights_web_page,
        req_origin='SFO',
        req_destination='ANC',
        req_date=datetime.date(2025, 1, 10),
    )
    message_history: list[ModelMessage] | None = None
    usage: Usage = Usage()
    
    # Main application loop - continue until user books or no flights found
    while True:
        # Run the search agent with our parameters
        result = await search_agent.run(
            f'Find me a flight from {deps.req_origin} to {deps.req_destination} on {deps.req_date}',
            deps=deps,
            usage=usage,
            message_history=message_history,
            usage_limits=usage_limits,
        )
        
        # Process the result
        if isinstance(result.data, NoFlightFound):
            print('No flight found')
            break
        else:
            flight = result.data
            print(f'Flight found: {flight}')
            answer = Prompt.ask(
                'Do you want to buy this flight, or keep searching? (buy/*search)',
                choices=['buy', 'search', ''],
                show_choices=False,
            )
            if answer == 'buy':
                # If user wants to book, collect seat preference and complete purchase
                seat = await find_seat(usage)
                await buy_tickets(flight, seat)
                break
            else:
                # Continue searching with context from previous interaction
                # This helps the agent understand we want different results
                message_history = result.all_messages(
                    result_tool_return_content='Please suggest another flight'
                )
```

----------------------------------------

TITLE: Generating Knowledge Graph Using Together AI API
DESCRIPTION: Function that calls the Together AI API with Meta-Llama-3.1-70B model in strict JSON mode to generate a knowledge graph based on input text, using the predefined schema.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# Call the LLM with the JSON schema
def generate_graph(input) -> KnowledgeGraph:
    together = Together(api_key = TOGETHER_API_KEY)

    extract = together.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": f"Build a knowledge graph to explain: {input}. Only answer in JSON.",
            }
        ],
        model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        response_format={
            "type": "json_object",
            "schema": KnowledgeGraph.model_json_schema(),
        },
    )

    output = json.loads(extract.choices[0].message.content)
    return output
```

----------------------------------------

TITLE: Implementing a Multi-Agent System with Specialized Roles
DESCRIPTION: Demonstrates how to create a team of specialized agents that work together, including a web search agent, a finance agent for retrieving financial data, and a coordinator agent that delegates tasks. This approach combines specialized capabilities for more complex tasks.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
from agno.agent import Agent
from agno.models.together import Together
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

# Create a specialized agent for web searches
web_agent = Agent(
    name="Web Agent",
    role="Search the web for information",
    # Using the larger Llama 3.3 (70B) model for better performance
    model=Together(id="meta-llama/Llama-3.3-70B-Instruct-Turbo", max_tokens=4096),
    tools=[DuckDuckGoTools()],
    instructions="Always include sources",
    show_tool_calls=True,
    markdown=True,
)

# Create a specialized agent for financial data
finance_agent = Agent(
    name="Finance Agent",
    role="Get financial data",
    model=Together(id="meta-llama/Llama-3.3-70B-Instruct-Turbo", max_tokens=4096),
    # Financial tools for stock data, analyst recommendations, and company info
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    instructions="Use tables to display data",
    show_tool_calls=True,
    markdown=True,
)

# Create a coordinator agent that manages the team
agent_team = Agent(
    # Provide the specialized agents as a team
    team=[web_agent, finance_agent],
    # The coordinator also uses a powerful model
    model=Together(id="meta-llama/Llama-3.3-70B-Instruct-Turbo", max_tokens=4096),
    # Instructions for the final output
    instructions=["Always include sources", "Use tables to display data"],
    show_tool_calls=True,
    markdown=True,
)

agent_team.print_response("What's the market outlook and financial performance of AI semiconductor companies?", stream=True)
```

----------------------------------------

TITLE: Setting up the Planner
DESCRIPTION: Configures the planner responsible for breaking down complex queries into steps. It uses a custom prompt and the Together AI Llama model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
from langchain_core.prompts import ChatPromptTemplate

planner_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """For the given objective, come up with a simple step by step plan. \
This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \
The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.""",
        ),
        ("placeholder", "{messages}"),
    ]
)

planner = planner_prompt | ChatTogether(model="meta-llama/Llama-3.3-70B-Instruct-Turbo").with_structured_output(Plan)
```

----------------------------------------

TITLE: Building an Agent with Specialized Knowledge Base for Thai Cuisine
DESCRIPTION: Creates an agent with a specialized knowledge base for Thai recipes by loading a PDF into a vector database. The agent prioritizes its knowledge base while still being able to search the web for supplementary information.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Agno/Agents_Agno.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Import additional components for knowledge base functionality
from agno.agent import Agent
from agno.models.together import Together
from agno.embedder.together import TogetherEmbedder
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

# Create an agent with a specialized knowledge base for Thai recipes
agent = Agent(
    # Use Together AI's Llama 3.1 model
    model=Together(id="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo", max_tokens=4096),
    # Define the agent's role
    description="You are a Thai cuisine expert!",
    # Give the agent specific instructions on how to use its knowledge
    instructions=[
        "Search your knowledge base for Thai recipes.",
        "If the question is better suited for the web, search the web to fill in gaps.",
        "Prefer the information in your knowledge base over the web results."
    ],
    # Set up the knowledge base from a PDF URL
    knowledge=PDFUrlKnowledgeBase(
        urls=["https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"],
        # Configure the vector database for semantic search
        vector_db=LanceDb(
            uri="tmp/lancedb",
            table_name="recipes",
            search_type=SearchType.hybrid,  # Use hybrid search (keyword + semantic)
            # Use Together AI's embedding model for vectorizing text
            embedder=TogetherEmbedder(id="togethercomputer/m2-bert-80M-32k-retrieval"),
        ),
    ),
    # Add web search capability as a backup
    tools=[DuckDuckGoTools()],
    # Display tool usage in the output
    show_tool_calls=True,
    markdown=True
)

# Load the knowledge base (only needs to be done once)
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response("How do I make chicken and galangal in coconut milk soup", stream=True)
agent.print_response("What is the history of Thai curry?", stream=True)
```

----------------------------------------

TITLE: Formatting Retrieved Chunks for Context
DESCRIPTION: Combines the top 5 retrieved chunks into a formatted string to provide context for the language model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_15

LANGUAGE: python
CODE:
```
# Lets add the top 5 documents to a string

formatted_chunks = ''

for i, chunk in enumerate(retrieved_chunks):
    formatted_chunks += f"Context {i+1}: {chunk}\n"

print(formatted_chunks)
```

----------------------------------------

TITLE: Querying Mistral Small Model with Augmented Prompt via Together API in Python
DESCRIPTION: This snippet demonstrates the final step of the thinking augmented generation process. It uses the Mistral Small model with a prompt that includes both the original question and the generated thought process.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
answer = client.chat.completions.create(
  model="mistralai/Mistral-Small-24B-Instruct-2501",
  messages=[{"role": "user", 
             "content": PROMPT_TEMPLATE.format(question = question,
                                               thinking_tokens=thought.choices[0].message.content
                                               )}],
)

print(answer.choices[0].message.content)
```

----------------------------------------

TITLE: Generic Router Implementation - Flexible Routing
DESCRIPTION: Provides a generic implementation of the router workflow that can handle arbitrary routes and models
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
def router_workflow(input_query: str, routes : Dict[str, str]) -> str:
    ROUTER_PROMPT = """Given a user prompt/query: {user_query}, select the best option out of the following routes:
    {routes}. Answer only in JSON format."""

    class Schema(BaseModel):
        route: Literal[tuple(routes.keys())]
    
        reason: str = Field(
            description="Short one-liner explanation why this route was selected for the task in the prompt/query."
        )

    selected_route = JSON_llm(ROUTER_PROMPT.format(user_query=input_query, routes=routes), Schema)
    print(f"Selcted route:{selected_route['route']}\nReason: {selected_route['reason']}\n")

    response = run_llm(user_prompt= input_query, model = selected_route['route'])
    print(f"Response: {response}\n")
    
    return response
```

----------------------------------------

TITLE: Implementing the Orchestrator Workflow Function
DESCRIPTION: Defines the core orchestrator workflow function that uses an orchestrator model to break down tasks, assigns subtasks to worker models, and executes them in parallel.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
class Task(BaseModel):
    type: Literal["plan", "code/solve", "test"]
    description: str

class TaskList(BaseModel):
    analysis: str
    tasks: List[Task]  = Field(..., default_factory=list)

async def orchestrator_workflow(task : str, orchestrator_prompt : str, worker_prompt : str): 
    """Use a orchestrator model to break down a task into sub-tasks and then use worker models to generate and return responses."""

    # Use orchestrator model to break the task up into sub-tasks
    orchestrator_response = JSON_llm(orchestrator_prompt.format(task=task), schema=TaskList)
 
    # Parse orchestrator response
    analysis = orchestrator_response["analysis"]
    tasks= orchestrator_response["tasks"]

    print("\n=== ORCHESTRATOR OUTPUT ===")
    print(f"\nANALYSIS:\n{analysis}")
    print(f"\nTASKS:\n{json.dumps(tasks, indent=2)}")

    worker_model =  ["Qwen/Qwen2.5-Coder-32B-Instruct"]*len(tasks)

    # Gather intermediate responses from worker models
    return tasks , await asyncio.gather(*[run_llm_parallel(user_prompt=worker_prompt.format(original_task=task, task_type=task_info['type'], task_description=task_info['description']), model=model) for task_info, model in zip(tasks,worker_model)])
```

----------------------------------------

TITLE: Creating Document Chunks in Python
DESCRIPTION: Actor task that splits document content into chunks with specified size and overlap parameters.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
@actor.task(cache=True, cache_version="0.2")
def create_chunks(document: Document, chunk_size: int, overlap: int) -> Document:
    if document.content:
        content_chunks = [
            document.content[i : i + chunk_size]
            for i in range(0, len(document.content), chunk_size - overlap)
        ]
        document.chunks = content_chunks
    return document
```

----------------------------------------

TITLE: Executing the Orchestrator Workflow with a Sample Task
DESCRIPTION: Runs the orchestrator workflow with a sample task to demonstrate how it breaks down the task and processes the subtasks in parallel.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
task = """Write a program that prints the next 20 leap years.
"""

tasks, worker_resp = await orchestrator_workflow(task, orchestrator_prompt=ORCHESTRATOR_PROMPT, worker_prompt=WORKER_PROMPT)
```

----------------------------------------

TITLE: Implementing Text Summarization with Together API
DESCRIPTION: Defines a function to generate a summary of text using the Together API client, sending a formatted prompt to the Meta-Llama-3.1-70B-Instruct-Turbo model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
from together import Together

client = Together(api_key = "__TOGETHER_API_KEY__")


def summarize(text: str, prompt: str) -> str:
    response = client.chat.completions.create(
        model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        messages=[{"role": "user", "content": prompt.format(full_text = text)}],
    )

    return response.choices[0].message.content
```

----------------------------------------

TITLE: Defining Fine-tuning Job Function for Summarization in Python
DESCRIPTION: This function sends a fine-tuning job to the Together platform. It sets up parameters for the fine-tuning process, including model selection, epochs, learning rate, and dataset file. It supports both synthetic and GovReport datasets.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
def send_ft_job(client,
                model="meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference",
                n_epochs=4,
                run_name='1113-summarization-long-context-finetune',
                train_on_inputs=False,
                learning_rate=6e-5,
                filename=None,
                summarization_file_id=None):
    """
    Sends a fine-tuning job to the client.
    Parameters:
        client (object): The client object to interact with the API.
        model (str): The model to be fine-tuned. Default is "meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference".
        n_epochs (int): Number of epochs for fine-tuning. Default is 4.
        run_name (str): The name for the run. Default is '1113-summarization-long-context-finetune'.
        train_on_inputs (bool): Whether to train on inputs. Default is False.
        learning_rate (float): The learning rate for fine-tuning. Default is 6e-5.
        filename (str, optional): The filename to upload for fine-tuning. Default is None.
        summarization_file_id (str, optional): The file ID for summarization. Must be provided if filename is None.
    Returns:
        str: The ID of the fine-tuning job.
    Raises:
        AssertionError: If neither filename nor summarization_file_id is provided.
    """
    if filename:
        response = client.files.upload(filename, check=True)
        summarization_file_id = response.id
    else:
        assert summarization_file_id is not None, "provide summarization_file_id"

    response = client.fine_tuning.create(
        training_file = summarization_file_id,
        model = model,
        n_epochs = n_epochs,
        n_checkpoints = 1,
        batch_size = "max",
        learning_rate = learning_rate,
        warmup_ratio = 0.05,
        suffix=run_name,
        wandb_api_key = WANDB_API_KEY,
        lora=True,
        lora_r=32,
        lora_alpha=64,
        lora_dropout=0.05,
        train_on_inputs=train_on_inputs,
    )
    summarization_fine_tuning_job_id = response.id
    return summarization_fine_tuning_job_id
```

----------------------------------------

TITLE: Generating Conditional Image from Image-to-Image Search Result in Python
DESCRIPTION: Creates a holiday cartoon version of the image retrieved through image-to-image search using the generate_image function and FLUX model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
# Generate a holiday cartoon version of the retrieved image
generated_image_2 = generate_image(image_prompt="Create a cute holiday cartoon version of this image.", retrieved_image = image_2_image)
```

----------------------------------------

TITLE: Implementing Helper Functions for LLM Calls
DESCRIPTION: Defines helper functions for making both regular and JSON-structured calls to language models with the Together API, handling prompts and responses.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Simple LLM call helper function
def run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):
    """ Run the language model with the given user prompt and system prompt. """
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": user_prompt})
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.7,
        max_tokens=4000,        
    )

    return response.choices[0].message.content

# Simple JSON mode LLM call helper function
def JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):
    """ Run a language model with the given user prompt and system prompt, and return a structured JSON object. """
    try:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": user_prompt})
        
        extract = client.chat.completions.create(
            messages=messages,
            model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            response_format={
                "type": "json_object",
                "schema": schema.model_json_schema(),
            },
        )
        
        response = json.loads(extract.choices[0].message.content)
        return response
        
    except ValidationError as e:
        raise ValueError(f"Schema validation failed: {str(e)}")

```

----------------------------------------

TITLE: Generating Embeddings for Text Chunks
DESCRIPTION: A function that uses the Together AI API to generate vector embeddings for text chunks using the BGE large English embedding model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
from typing import List

import numpy as np

def generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:
    """Generate embeddings from Together python library.

    Args:
        input_texts: a list of string input texts.
        model_api_string: str. An API string for a specific embedding model of your choice.

    Returns:
        embeddings_list: a list of embeddings. Each element corresponds to the each input text.
    """
    outputs = client.embeddings.create(
        input=input_texts,
        model=model_api_string,
    )
    return np.array([x.embedding for x in outputs.data])
```

----------------------------------------

TITLE: Compiling Document Strings from Hybrid Results in Python
DESCRIPTION: This snippet retrieves documents based on the indices from the hybrid results and prepares a string containing these documents, which will be utilized for further processing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_38

LANGUAGE: python
CODE:
```
[contextual_chunks[index] for index in hybrid_top_k[1]]
```

----------------------------------------

TITLE: Initializing JinaCLIP Model for Image Embedding in Python
DESCRIPTION: Sets up the JinaCLIP model to generate vector embeddings for images. This model unifies text and image representations for multimodal search.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
from transformers import AutoModel

# Initialize the model
model = AutoModel.from_pretrained('jinaai/jina-clip-v1', trust_remote_code=True)

# Encode text and images
image_embeddings = model.encode_image(links)  # also accepts PIL.image, local filenames, dataURI

image_embeddings.shape
```

----------------------------------------

TITLE: Implementing Agent, Rewrite, and Generate Functions
DESCRIPTION: This code defines three key functions for the agentic RAG system: agent (decides whether to retrieve or end), rewrite (transforms the query for better results), and generate (produces the final answer). These functions use the ChatTogether model for language processing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
### Nodes
def agent(state):
    """
    Invokes the agent model to generate a response based on the current state. Given
    the question, it will decide to retrieve using the retriever tool, or simply end.

    Args:
        state (messages): The current state

    Returns:
        dict: The updated state with the agent response appended to messages
    """
    print("---CALL AGENT---")
    messages = state["messages"]
    
    model = ChatTogether(api_key=os.getenv("TOGETHER_API_KEY"), model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",temperature=0, streaming=True)
    model = model.bind_tools(tools)
    response = model.invoke(messages)
    # We return a list, because this will get added to the existing list
    return {"messages": [response]}


def rewrite(state):
    """
    Transform the query to produce a better question.

    Args:
        state (messages): The current state

    Returns:
        dict: The updated state with re-phrased question
    """

    print("---TRANSFORM QUERY---")
    messages = state["messages"]
    question = messages[0].content

    msg = [
        HumanMessage(
            content=f""" \n 
    Look at the input and try to reason about the underlying semantic intent / meaning. \n 
    Here is the initial question:
    \n ------- \n
    {question} 
    \n ------- \n
    Formulate an improved question: """,
        )
    ]

    # Grader
    model = ChatTogether(api_key=os.getenv("TOGETHER_API_KEY"), model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo", temperature=0, streaming=True)
    response = model.invoke(msg)
    return {"messages": [response]}


def generate(state):
    """
    Generate answer

    Args:
        state (messages): The current state

    Returns:
         dict: The updated state with re-phrased question
    """
    print("---GENERATE---")
    messages = state["messages"]
    question = messages[0].content
    last_message = messages[-1]

    docs = last_message.content

    # Prompt
    prompt = hub.pull("rlm/rag-prompt")

    # LLM
    llm = ChatTogether(api_key=os.getenv("TOGETHER_API_KEY"), model="deepseek-ai/DeepSeek-R1", temperature=0.6,streaming=True)

    # Post-processing
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # Chain
    rag_chain = prompt | llm | StrOutputParser()

    # Run
    response = rag_chain.invoke({"context": docs, "question": question})
    return {"messages": [response]}


print("*" * 20 + "Prompt[rlm/rag-prompt]" + "*" * 20)
prompt = hub.pull("rlm/rag-prompt").pretty_print()  # Show what the prompt looks like
```

----------------------------------------

TITLE: Swapping Between LoRA Adapters for Inference
DESCRIPTION: Demonstrates how to loop through multiple LoRA adapters and perform inference with the same prompt. This showcases the flexibility of LoRA for quickly switching between different finetuned versions without reloading the full model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
# Loop over different LoRA fine-tunes and call with same query

for adapter in LoRA_adapters:
    
    response = client.chat.completions.create(
    model = adapter,
    messages=[
        {
            "role": "user",
            "content": "Write a short haiku about elephants.",
        }
    ],
    max_tokens=124,
    temperature=0.7,
    )

    print(f"Response from {adapter}:\n")

    print(response.choices[0].message.content)

    print('\n'+20*'######'+'\n')
```

----------------------------------------

TITLE: Initial Fine-tuning on Core Function Calling Categories
DESCRIPTION: Performs the first step of fine-tuning using LoRA on the Llama-3.2-1B-Instruct model with core function calling categories.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
ft_fc1 = client.fine_tuning.create(
    training_file = resp_fc1.id, # function_calling_1.jsonl
    model = 'meta-llama/Llama-3.2-1B-Instruct',
    n_epochs = 1,
    n_checkpoints = 1,
    batch_size = 16,
    lora=True,
    warmup_ratio=0.1,
    wandb_api_key=WANDB_API_KEY,
)
print(ft_fc1.id)
```

----------------------------------------

TITLE: Evaluating and Comparing Model Performance with ROUGE Metrics in Python
DESCRIPTION: This code iterates through different model configurations (baseline and fine-tuned) on two datasets (Synthetic and GovReport), computes ROUGE-L scores for each, and prints the average score. It demonstrates how to systematically compare model performance across different configurations.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
for k, (predictions_, test_items_) in zip(
    ["Synthetic Baseline:", "Synthetic Fine-tune:", "GovReport Baseline:", "GovReport Fine-tune:"],
    [(predictions_baseline_syn, test_items), (predictions_ft_syn, test_items),
     (predictions_baseline_govreport, test_items_govreport), (predictions_ft_govreport, test_items_govreport),]
):

    rouge_scores = compute_rouge_l(predictions_, test_items_)
    print(k, f"ROUGE: {np.mean(rouge_scores):.2f}")
```

----------------------------------------

TITLE: Research Completeness Evaluation in Python
DESCRIPTION: This function evaluates if the current search results are sufficient or if more research is needed. It takes the research topic, current search results, a list of queries already used, Together AI client, models for planning and JSON parsing, and a dictionary of prompt templates as input. It returns a list of additional queries needed, or an empty list if the research is complete. It leverages LLMs to assess completeness and generate follow-up queries.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
async def evaluate_research_completeness(topic: str, results: SearchResults, queries: List[str],
                                       together_client: AsyncTogether, planning_model: str, json_model: str, prompts: dict) -> list[str]:
    """
    Evaluate if the current search results are sufficient or if more research is needed.

    Args:
        topic: The research topic
        results: Current search results
        queries: List of queries already used
        together_client: The Together AI client for LLM operations
        planning_model: Model to use for evaluation
        json_model: Model to use for JSON parsing
        prompts: Dictionary of prompt templates

    Returns:
        List of additional queries needed or empty list if research is complete
    """
    # Format the search results for the LLM
    formatted_results = str(results)

    EVALUATION_PROMPT = prompts["evaluation_prompt"]

    evaluation_response = await together_client.chat.completions.create(
        model=planning_model,
        messages=[
            {"role": "system", "content": EVALUATION_PROMPT},
            {"role": "user", "content": (
                f"<Research Topic>{topic}</Research Topic>\n\n"
                f"<Search Queries Used>{queries}</Search Queries Used>\n\n"
                f"<Current Search Results>{formatted_results}</Current Search Results>"
            )}
        ]
    )
    evaluation = evaluation_response.choices[0].message.content

    print("================================================\n\n")
    print(f"Evaluation:\n\n {evaluation}")

    EVALUATION_PARSING_PROMPT = prompts["evaluation_parsing_prompt"]

    json_response = await together_client.chat.completions.create(
        model=json_model,
        messages=[
            {"role": "system", "content": EVALUATION_PARSING_PROMPT},
            {"role": "user", "content": f"Evaluation to be parsed: {evaluation}"}
        ],
        response_format={"type": "json_object", "schema": ResearchPlan.model_json_schema()}
    )

    response_json = json_response.choices[0].message.content
    evaluation = json.loads(response_json)
    return evaluation["queries"]
```

----------------------------------------

TITLE: Creating a Summarization Prompt Template
DESCRIPTION: Defines a comprehensive prompt template for instructing an LLM to generate a concise and detailed summary of the provided text, with specific guidelines for format and content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
SUMMARIZATION_PROMPT = """As a professional summarizer, create a concise and comprehensive summary of the provided text, be it an article, post, conversation, or passage, while adhering to these guidelines:

1. Craft a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness.

2. Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects.

3. Rely strictly on the provided text, without including external information.

4. Format the summary into a paragraph form for easy understanding.

The input may be unstructured or messy, sourced from PDFs or web pages. Disregard irrelevant information or formatting issues.

{full_text}
"""
```

----------------------------------------

TITLE: Configuring Research Pipeline Parameters in Python
DESCRIPTION: Sets up the core configuration parameters for the research pipeline, including budget (number of research cycles), query limits, source limits, and token limits for the final report.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_0

LANGUAGE: python
CODE:
```
# Parameters controlling research depth and breadth
budget = 2  # Number of research refinement cycles to perform (in addition to the initial search operation)
max_queries = 2  # Maximum number of search queries per research cycle
max_sources = 10  # Maximum number of sources to include in final synthesis
max_tokens = 8192 # Maximum number of tokens in the generated report
```

----------------------------------------

TITLE: Printing Reordered Chunks Based on RRF Score in Python
DESCRIPTION: This snippet iterates through the combined results from the RRF output and prints the indices along with their corresponding contextual chunks, showcasing the new order based on relevance.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_36

LANGUAGE: python
CODE:
```
for index in hybrid_top_k[1]:
  print(f"Chunk Index {index} : {contextual_chunks[index]}")
```

----------------------------------------

TITLE: Creating Seat Selection Agent with PydanticAI
DESCRIPTION: This snippet defines a separate agent for handling natural language seat preferences. It includes models for seat preference and failure cases, and sets up the agent with appropriate prompts.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
class SeatPreference(BaseModel):
    row: int = Field(ge=1, le=30)
    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']


class Failed(BaseModel):
    """Unable to extract a seat selection."""


# This agent is responsible for extracting the user's seat selection
seat_preference_agent = Agent[None, SeatPreference | Failed](
    model=llm,
    result_type=SeatPreference | Failed,  # type: ignore
    system_prompt=(
        "Extract the user's seat preference. "
        'Seats A and F are window seats. '
        'Row 1 is the front row and has extra leg room. '
        'Rows 14, and 20 also have extra leg room. '
    ),
)
```

----------------------------------------

TITLE: Creating Embedding Generation Function Using Together API
DESCRIPTION: Defines a function to generate embeddings for input texts using the Together API. The function takes a list of texts and a model identifier, returning a list of vector embeddings.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_19

LANGUAGE: python
CODE:
```
from typing import List
import together
import numpy as np

def generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:
    """Generate embeddings from Together python library.

    Args:
        input_texts: a list of string input texts.
        model_api_string: str. An API string for a specific embedding model of your choice.

    Returns:
        embeddings_list: a list of embeddings. Each element corresponds to the each input text.
    """
    outputs = client.embeddings.create(
        input=input_texts,
        model=model_api_string,
    )
    return [x.embedding for x in outputs.data]
```

----------------------------------------

TITLE: Implementing Reciprocal Rank Fusion (RRF) Algorithm in Python
DESCRIPTION: This function fuses rankings from multiple information retrieval systems using the RRF method. It calculates scores for ranked documents from each system and returns a combined list of documents sorted by their score.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_34

LANGUAGE: python
CODE:
```
from collections import defaultdict

def reciprocal_rank_fusion(*list_of_list_ranks_system, K=60):
    """
    Fuse rank from multiple IR systems using Reciprocal Rank Fusion.

    Args:
    * list_of_list_ranks_system: Ranked results from different IR system.
    K (int): A constant used in the RRF formula (default is 60).

    Returns:
    Tuple of list of sorted documents by score and sorted documents
    """
    # Dictionary to store RRF mapping
    rrf_map = defaultdict(float)

    # Calculate RRF score for each result in each list
    for rank_list in list_of_list_ranks_system:
        for rank, item in enumerate(rank_list, 1):
            rrf_map[item] += 1 / (rank + K)

    # Sort items based on their RRF scores in descending order
    sorted_items = sorted(rrf_map.items(), key=lambda x: x[1], reverse=True)

    # Return tuple of list of sorted documents by score and sorted documents
    return sorted_items, [item for item, score in sorted_items]
```

----------------------------------------

TITLE: Creating Visualization Function with GraphViz
DESCRIPTION: Defines a function to visualize the knowledge graph using GraphViz, transforming the JSON structure into a directed graph with labeled nodes and edges.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
from graphviz import Digraph

def visualize_knowledge_graph(kg):
    dot = Digraph(comment="Knowledge Graph", format='png')

    # Add nodes
    for node in kg['nodes']:
        dot.node(str(node['id']), node['label'])

    # Add edges
    for edge in kg['edges']:
        dot.edge(str(edge['source']), str(edge['target']), label=edge['label'])

    # Render the graph to a file and open it
    output_path = dot.render("knowledge_graph", view=True)
    print(f"Graph rendered and saved to {output_path}")
```

----------------------------------------

TITLE: Displaying the Generated Knowledge Graph JSON
DESCRIPTION: Outputs the JSON structure of the generated knowledge graph, showing the nodes and edges created by the LLM.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
# Lets see the knowlege graph components generated!
graph
```

----------------------------------------

TITLE: Creating Vector Embeddings and Index in Python
DESCRIPTION: Functions to generate embeddings using Together AI and store them in Milvus vector database with caching and retry functionality.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
from together import Together

def get_embedding(chunk: str, embedding_model: str):
    client = Together(
        api_key=os.getenv("TOGETHER_API_KEY")
    )
    outputs = client.embeddings.create(
        input=chunk,
        model=embedding_model,
    )
    return outputs.data[0].embedding


@actor.task(cache=True, cache_version="0.19", retries=5)
def create_vector_index(
    document: Document, embedding_model: str, local: bool = False
) -> Document:
    from pymilvus import DataType, MilvusClient

    if local:
        client = MilvusClient("test_milvus.db")
    else:
        try:
            client = MilvusClient(uri=os.getenv("MILVUS_URI"), token=os.getenv("MILVUS_TOKEN"))
        except Exception as e:
            raise FlyteRecoverableException(
                f"Failed to connect to Milvus: {e}"
            )

    collection_name = "paul_graham_collection"

    if not client.has_collection(collection_name):
        schema = client.create_schema()
        schema.add_field(
            "id", DataType.INT64, is_primary=True, auto_id=True
```

----------------------------------------

TITLE: Executing Tool Calls and Updating Context
DESCRIPTION: Runs the tools selected by the LLM using the arguments it provided, and updates the conversation context with the results. This allows the LLM to use the data from the tool execution.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
tool_run = th.run_tools(response)
messages += tool_run

# Here's what the messages variable contains now
print(messages)
```

----------------------------------------

TITLE: Inference with Fine-tuned LLM on Together AI Platform
DESCRIPTION: Demonstrates how to use a previously fine-tuned model for inference on the Together AI platform. The code specifies the model name and sends a user prompt requesting the model to respond in the style of Crush from Finding Nemo.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
finetuned_model = "zainhas/Meta-Llama-3.1-8B-Instruct-Reference-helpsteer2_dpo_training_continuing_sft-cf1147c8"#dpo_training.output_name #this is the name of the finetuned model

user_prompt = """I want you to act like Crush the sea turtle from Finding Nemo. 
I want you to respond and answer like Crush using the tone, manner and vocabulary Crush would use. Do not write any explanations. 
Only answer like Crush. 
You must know all of the knowledge of Crush.
"""

response = client.chat.completions.create(
    model = finetuned_model,
    messages=[
        {
            "role": "user",
            "content": user_prompt,
        }
    ]
)

print(response.choices[0].message.content)
```

----------------------------------------

TITLE: Creating a ReAct Agent
DESCRIPTION: Sets up a ReAct agent that can use tools and reason about their outputs. It uses the previously defined LLM and tools.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
from langchain import hub

from langgraph.prebuilt import create_react_agent

# Choose the LLM that will drive the agent
prompt = "You are a helpful assistant."
agent_executor = create_react_agent(llm, tools, prompt=prompt)
```

----------------------------------------

TITLE: Building BM25 Search Index Using Python
DESCRIPTION: This snippet imports the bm25s library to create a BM25 model and index a corpus of contextual chunks, enabling efficient lexical searching based on the words present in the query.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_29

LANGUAGE: python
CODE:
```
import bm25s

# Create the BM25 model and index the corpus
retriever = bm25s.BM25(corpus=contextual_chunks)
retriever.index(bm25s.tokenize(contextual_chunks))
```

----------------------------------------

TITLE: Defining a Workflow for Index Building
DESCRIPTION: This code defines a Flyte workflow `build_indices_wf` that orchestrates several tasks: parsing a main page, scraping content, creating chunks, generating context, creating a vector index, and building a BM25S index. The workflow utilizes `union.map_task` to execute tasks in parallel. The function also defines expected inputs and outputs using type annotations.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
import functools
from dataclasses import dataclass

from dotenv import load_dotenv

load_dotenv()  # Ensure the secret (together API key) is present in the .env file

BM25Index = Artifact(name="bm25s-index")
ContextualChunksJSON = Artifact(name="contextual-chunks-json")


@union.workflow
def build_indices_wf(
    base_url: str = "https://paulgraham.com/",
    articles_url: str = "articles.html",
    embedding_model: str = "BAAI/bge-large-en-v1.5",
    chunk_size: int = 250,
    overlap: int = 30,
    model: str = "deepseek-ai/DeepSeek-R1",
    local: bool = True,
) -> tuple[
    Annotated[FlyteDirectory, BM25Index], Annotated[FlyteFile, ContextualChunksJSON]
]:
    tocs = parse_main_page(base_url=base_url, articles_url=articles_url, local=local)
    scraped_content = union.map_task(scrape_pg_essays, concurrency=2)(document=tocs)
    chunks = union.map_task(
        functools.partial(create_chunks, chunk_size=chunk_size, overlap=overlap)
    )(document=scraped_content)
    contextual_chunks = union.map_task(functools.partial(generate_context, model=model))(document=chunks)
    union.map_task(
        functools.partial(
            create_vector_index, embedding_model=embedding_model, local=local
        ),
        concurrency=2
    )(document=contextual_chunks)
    bm25s_index, contextual_chunks_json_file = create_bm25s_index(
        documents=contextual_chunks
    )
    return bm25s_index, contextual_chunks_json_file
```

----------------------------------------

TITLE: Generating Image with LoRA Trigger Words
DESCRIPTION: Shows how to use trigger words to activate the LoRA fine-tune effectively. In this case, it adds a specific phrase to activate the tarot card style LoRA.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
prompt = "a baby panda eating bamboo in the style of TOK a trtcrd tarot style"

generated_image = generate_image(prompt, 
                                 lora1="https://huggingface.co/multimodalart/flux-tarot-v1",
                                 scale1=1.0
                                 )

Image(url=generated_image, width=512, height=384)
```

----------------------------------------

TITLE: Applying Transformation to CoQA Dataset
DESCRIPTION: Applies the mapping function to transform the entire CoQA dataset into the required chat format for fine-tuning. This creates a new dataset with only the 'messages' column containing properly formatted conversations.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# transform the data using the mapping function

train_messages = coqa_dataset["train"].map(map_fields, remove_columns=coqa_dataset["train"].column_names)
```

----------------------------------------

TITLE: Implementing Retriever Function
DESCRIPTION: Creates a function to retrieve top-k similar items based on cosine similarity of embeddings.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
def retreive(query: str, top_k: int = 5, index: np.ndarray = None) -> List[int]:
    """
    Retrieve the top-k most similar items from an index based on a query.
    Args:
        query (str): The query string to search for.
        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.
        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.
    Returns:
        List[int]: A list of indices corresponding to the top-k most similar items in the index.
    """
    
    query_embedding = generate_embeddings([query], 'BAAI/bge-base-en-v1.5')[0]
    similarity_scores = cosine_similarity([query_embedding], index)

    return np.argsort(-similarity_scores)[0][:top_k]
```

----------------------------------------

TITLE: Creating a ReAct Agent for Fact-Checking
DESCRIPTION: Sets up a DSPy ReAct agent with a signature to find relevant Wikipedia titles for fact verification, using the previously defined search and lookup tools.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
instructions = "Find all Wikipedia titles relevant to verifying (or refuting) the claim."
signature = dspy.Signature("claim -> titles: list[str]", instructions)
react = dspy.ReAct(signature, tools=[search_wikipedia, lookup_wikipedia], max_iters=20)
```

----------------------------------------

TITLE: Setting Up Wikipedia Search with ColBERTv2
DESCRIPTION: Configures a function to search Wikipedia using ColBERTv2, saving retrieved documents in a dictionary for later access.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
DOCS = {}

def search(query: str, k: int) -> list[str]:
    results = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')(query, k=k)
    results = [x['text'] for x in results]

    for result in results:
        title, text = result.split(" | ", 1)
        DOCS[title] = text

    return results
```

----------------------------------------

TITLE: Executing Parallel Search with Initial Queries in Python
DESCRIPTION: This line calls the perform_search function with the initial set of research queries. It starts the parallel search operation that will gather comprehensive information across multiple topics related to the research subject.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
initial_results=await perform_search(initial_queries, tavily_client, prompts, together_client, summary_model)
```

----------------------------------------

TITLE: LLM Helper Functions Implementation
DESCRIPTION: Implementation of helper functions for running LLM queries, including standard text responses and JSON-structured outputs.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
def run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):
    " Run the language model with the given user prompt and system prompt. "
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": user_prompt})
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.7,
        max_tokens=4000,        
    )

    return response.choices[0].message.content

def JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):
    " Run a language model with the given user prompt and system prompt, and return a structured JSON object. "
    try:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": user_prompt})
        
        extract = client.chat.completions.create(
            messages=messages,
            model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            response_format={
                "type": "json_object",
                "schema": schema.model_json_schema(),
            },
        )
        
        response = json.loads(extract.choices[0].message.content)
        return response
        
    except ValidationError as e:
        raise ValueError(f"Schema validation failed: {str(e)}")
```

----------------------------------------

TITLE: Evaluating Finetuned Models using Python
DESCRIPTION: This code snippet iterates over a list of model names, retrieves answers from each model, computes evaluation metrics (Exact Match and F1), and prints the results. It is designed to facilitate the evaluation of the performance of finetuned models compared to their original versions.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
models_names = [
    "meta-llama/Meta-Llama-3.1-8B-Instruct-Reference",
    "zainhas/Meta-Llama-3.1-8B-Instruct-Reference-my-demo-finetune-4224205a", # finetuned model goes here once deployed
]

for model_name in models_names:
    print(model_name)
    answers = get_model_answers(model_name)
    em_metric, f1_metric = get_metrics(answers)
    print(f"EM: {em_metric}, F1: {f1_metric}")
```

----------------------------------------

TITLE: Performing Inference with LoRA Finetuned Model
DESCRIPTION: Demonstrates how to use the finetuned model with LoRA adapter for inference. The code sends a simple question to the model and prints the response, appending '-adapter' to the model name to indicate LoRA inference.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
model_name = ft_resp.output_name
user_prompt = "What is the capital of the France?"

response = client.chat.completions.create(
    model = model_name + '-adapter',
    messages=[
        {
            "role": "user",
            "content": user_prompt,
        }
    ],
    max_tokens=124,
    temperature=0.7,
)

print(response.choices[0].message.content)
```

----------------------------------------

TITLE: Implementing Search and Lookup Tools for ReAct Agent
DESCRIPTION: Defines two tools for the ReAct agent: search_wikipedia retrieves multiple results for a query, and lookup_wikipedia retrieves specific Wikipedia page content by title.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
# Now, let's use the search function to define two tools for our ReAct agent:

def search_wikipedia(query: str) -> list[str]:
    """Returns top-5 results and then the titles of the top-5 to top-30 results."""

    topK = search(query, 30)
    titles, topK = [f"`{x.split(' | ')[0]}`" for x in topK[5:30]], topK[:5]
    return topK + [f"Other retrieved pages have titles: {', '.join(titles)}."] 

def lookup_wikipedia(title: str) -> str:
    """Returns the text of the Wikipedia page, if it exists."""

    if title in DOCS:
        return DOCS[title]

    results = [x for x in search(title, 10) if x.startswith(title + " | ")]
    if not results:
        return f"No Wikipedia page found for title: {title}"
    return results[0]
```

----------------------------------------

TITLE: Loading HoVer Dataset for Multi-Hop Fact Checking
DESCRIPTION: Loads examples from the HoVer dataset containing complex multi-hop claims, filtering for 3-hop examples and splitting into train, dev, and test sets.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
import random
from dspy.datasets import DataLoader

kwargs = dict(fields=("claim", "supporting_facts", "hpqa_id", "num_hops"), input_keys=("claim",))
hover = DataLoader().from_huggingface(dataset_name="hover-nlp/hover", split="train", trust_remote_code=True, **kwargs)

hpqa_ids = set()
hover = [
    dspy.Example(claim=x.claim, titles=list(set([y["key"] for y in x.supporting_facts]))).with_inputs("claim")
    for x in hover
    if x["num_hops"] == 3 and x["hpqa_id"] not in hpqa_ids and not hpqa_ids.add(x["hpqa_id"])
]

random.Random(0).shuffle(hover)
trainset, devset, testset = hover[:100], hover[100:200], hover[650:]
```

----------------------------------------

TITLE: Generating Context with Together AI in Python
DESCRIPTION: Actor task that uses Together AI to generate context for each document chunk with caching enabled.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
@actor.task(cache=True, cache_version="0.4")
def generate_context(document: Document, model: str) -> Document:
    from together import Together

    CONTEXTUAL_RAG_PROMPT = """
Given the document below, we want to explain what the chunk captures in the document.

{WHOLE_DOCUMENT}

Here is the chunk we want to explain:

{CHUNK_CONTENT}

Answer ONLY with a succinct explanation of the meaning of the chunk in the context of the whole document above.
"""

    client = Together(api_key=os.getenv("TOGETHER_API_KEY"))

    contextual_chunks = [
        f"{response.choices[0].message.content} {chunk}"
        for chunk in (document.chunks or [])
        for response in [
            client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": CONTEXTUAL_RAG_PROMPT.format(
                            WHOLE_DOCUMENT=document.content,
                            CHUNK_CONTENT=chunk,
                        ),
                    }
                ],
                temperature=1,
            )
        ]
    ]

    # Assign the contextual chunks back to the document
    document.contextual_chunks = contextual_chunks if contextual_chunks else None
    return document
```

----------------------------------------

TITLE: Defining the Context Generation Function
DESCRIPTION: Creates a function that calls the Llama-3.2-3B-Instruct-Turbo model via the Together API to generate contextual explanations for each chunk.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_15

LANGUAGE: python
CODE:
```
def generate_context(prompt: str):
    """
    Generates a contextual response based on the given prompt using the specified language model.
    Args:
        prompt (str): The input prompt to generate a response for.
    Returns:
        str: The generated response content from the language model.
    """
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.2-3B-Instruct-Turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=1
    )
    return response.choices[0].message.content
```

----------------------------------------

TITLE: Uploading Dataset to Together AI
DESCRIPTION: Uploads the prepared JSONL file to Together AI's storage. The file ID returned will be used to reference this dataset when creating the fine-tuning job.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
## Upload the data to Together

train_file_resp = client.files.upload("coqa_prepared_train.jsonl", check=True)
print(f"Train file response: {train_file_resp.id}")
```

----------------------------------------

TITLE: Generating Story with RAG and Llama3
DESCRIPTION: Uses retrieved movie information to generate a story using Llama3 8B model via Together AI's chat completions API.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
client = Together(api_key = TOGETHER_API_KEY)

# Generate a story based on the top 10 most similar movies

response = client.chat.completions.create(
    model="meta-llama/Llama-3-8b-chat-hf",
    messages=[
      {"role": "system", "content": "You are a pulitzer award winning craftful story teller. Given only the overview of different plots you can weave together an interesting storyline."},
      {"role": "user", "content": f"Tell me a story about {titles}. Here is some information about them {overviews}"},
    ],
)
```

----------------------------------------

TITLE: Executing LLM Request with Tool Integration
DESCRIPTION: Sends a request to the LLM with available tools from Toolhouse, allowing the model to decide which tool to use. It includes a fix for compatibility issues with the image generation tool's description.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
# let's pull tools from Toolhouse
tools = th.get_tools(bundle="togethertoolhouse")
# change the value of the description field, when it's 'image_generation_flux'
# this is for compatibility reasons
custom_tools = [t for t in tools if t['function']['name'] == 'image_generation_flux']
custom_tools[0]['function']['description'] = "an image generation function"

# add the value back into the tools array overwriting the previous
tools = [t if t['function']['name'] != 'image_generation_flux' else custom_tools[0] for t in tools]

# now that we have fixed the tools array we can continue
response = client.chat.completions.create(
    model=MODEL,
    messages=messages,
    tool_choice="auto",
    tools=tools
)
print("The LLM autonomously decides to use the following tool(s) to achieve its goal of finding data on X/Twitter")
print("Tool selected: ", response.choices[0].message.tool_calls[0].function.name)
```

----------------------------------------

TITLE: Creating System Prompt for Aggregator Model
DESCRIPTION: Prepares a system prompt for the aggregator model that will synthesize the multiple intermediate responses into a final, comprehensive answer.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
# Gather all intermediate responses along with the system prompt
aggregator_system_prompt = """You have been provided with a set of responses from various open-source models to the latest user query.
Your task is to synthesize these responses into a single, high-quality response. It is crucial to critically evaluate the information
provided in these responses, recognizing that some of it may be biased or incorrect. Your response should not simply replicate the
given answers but should offer a refined, accurate, and comprehensive reply to the instruction. Ensure your response is well-structured,
coherent, and adheres to the highest standards of accuracy and reliability.

Responses from models:"""

print(aggregator_system_prompt + "\n" + "\n".join(f"{i+1}. {str(element)}" for i, element in enumerate(results)))
```

----------------------------------------

TITLE: Sorting Movies by Semantic Relevance
DESCRIPTION: Sorts movie indices based on their similarity scores to identify which movies are most semantically similar to the query.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
# Get the indices of the highest to lowest cosine similarity values
# This tell use which movies are most similar to the query

# Lookin at the results below we see that the movie at index 172 is the most similar to the query!
indices = np.argsort(-similarity_scores)

print(indices)
```

----------------------------------------

TITLE: Alternative Fine-tuning Without Continual Learning
DESCRIPTION: Performs fine-tuning on the second dataset without using the checkpoint from the first run, for comparison with the continual fine-tuning approach.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
alt_ft = client.fine_tuning.create(
    training_file = resp_fc2.id, # function_calling_2.jsonl
    model = 'meta-llama/Llama-3.2-1B-Instruct',
    n_epochs = 1,
    n_checkpoints = 1,
    batch_size = 16,
    lora=True,
    warmup_ratio=0.1,
    wandb_api_key=WANDB_API_KEY,
)

print(alt_ft.id)
```

----------------------------------------

TITLE: Defining the Contextual RAG Prompt Template
DESCRIPTION: Creates a prompt template used to generate contextual explanations for each chunk. The template includes placeholders for the entire document and the specific chunk being explained.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
# We want to generate a snippet explaining the relevance/importance of the chunk with
# full document in mind.

CONTEXTUAL_RAG_PROMPT = """
Given the document below, we want to explain what the chunk captures in the document.

{WHOLE_DOCUMENT}

Here is the chunk we want to explain:

{CHUNK_CONTENT}

Answer ONLY with a succinct explaination of the meaning of the chunk in the context of the whole document above.
"""
```

----------------------------------------

TITLE: Combining Results Using RRF in Python
DESCRIPTION: This snippet calls the reciprocal rank fusion function using the top k results retrieved from vector and BM25 queries to produce a combined list of ranked documents.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_35

LANGUAGE: python
CODE:
```
# Combine the lists using RRF
hybrid_top_k = reciprocal_rank_fusion(vector_top_k, bm25_top_k)
hybrid_top_k[1]
```

----------------------------------------

TITLE: Uploading Dataset Files to Together AI
DESCRIPTION: Uploads the training and validation dataset files to the Together AI cloud for use in fine-tuning. It sets 'check=True' to trigger a format check ensuring the data is in the correct format for DPO.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
dpo_train_file = client.files.upload(dpo_train_file_path, check=True)
dpo_validation_file = client.files.upload(dpo_validation_file_path, check=True)

print(f"Uploaded DPO training files: {dpo_train_file}")
print(f"Uploaded DPO validation files: {dpo_validation_file}")
```

----------------------------------------

TITLE: Setting Up Together API Client for LLM Access
DESCRIPTION: Imports necessary libraries and initializes the Together API clients (both synchronous and asynchronous) with an API key for accessing language models.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
# Import libraries
import json
import asyncio
import together
from together import AsyncTogether, Together

from typing import Any, Optional, Dict, List, Literal
from pydantic import Field, BaseModel, ValidationError

TOGETHER_API_KEY = "--Your API Key--"

client = Together(api_key= TOGETHER_API_KEY)
async_client = AsyncTogether(api_key= TOGETHER_API_KEY)
```

----------------------------------------

TITLE: Converting HelpSteer2-DPO Dataset to Preference Format
DESCRIPTION: Defines a function to convert the HelpSteer2-DPO dataset into a format suitable for Together AI preference fine-tuning, including input messages, preferred output, and non-preferred output.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
def convert_to_preference_dataset(dataset):
    """
    Converts the HelpSteer2-DPO dataset to a format suitable for together.ai preference fine-tuning.
    
    Returns a dataset with the preference format.
    """
    converted_dataset = {
        "train": [],
        "validation": []
    }
    
    for split in ["train", "validation"]:
        for example in dataset[split]:
            # Create the input messages
            messages = [
                {"role": "user", "content": example["prompt"]}
            ]
            
            # Create the preferred and non-preferred outputs
            preferred_output = [
                {"role": "assistant", "content": example["chosen_response"]}
            ]
            
            non_preferred_output = [
                {"role": "assistant", "content": example["rejected_response"]}
            ]
            
            # Add the converted example to the dataset
            converted_dataset[split].append({
                "input": {
                    "messages": messages
                },
                "preferred_output": preferred_output,
                "non_preferred_output": non_preferred_output
            })
    
    return converted_dataset
```

----------------------------------------

TITLE: Implementing Embedding Generation Function
DESCRIPTION: Creates a function to generate embeddings using Together AI's API for text inputs.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
from typing import List
import numpy as np

def generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:
    """Generate embeddings from Together python library.

    Args:
        input_texts: a list of string input texts.
        model_api_string: str. An API string for a specific embedding model of your choice.

    Returns:
        embeddings_list: a list of embeddings. Each element corresponds to the each input text.
    """
    together_client = together.Together(api_key = TOGETHER_API_KEY)
    outputs = together_client.embeddings.create(
        input=input_texts,
        model=model_api_string,
    )
    return np.array([x.embedding for x in outputs.data])
```

----------------------------------------

TITLE: Testing Vector Retrieval Function
DESCRIPTION: Tests the vector retrieval function with a specific query about floating point operations in the bill.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
vector_retrieval(query = "what is the maximum allowable floating point operation per second this bill allows?", top_k = 5, vector_index = embeddings, chunks = chunks)
```

----------------------------------------

TITLE: Retrieving Chunks for LLM Processing
DESCRIPTION: Retrieves relevant chunks based on a specific query for processing by a language model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
retrieved_chunks = vector_retrieval(query = "what is the maximum allowable floating point operation per second this bill allows?", top_k = 5, vector_index = embeddings, chunks = chunks)
```

----------------------------------------

TITLE: Displaying the Knowledge Graph Image in Notebook
DESCRIPTION: Uses IPython.display to show the generated knowledge graph image directly in the notebook interface.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
from IPython.display import Image, display

# Display the image of the knowledge graph
display(Image(filename='/content/knowledge_graph.png'))
```

----------------------------------------

TITLE: Implementing Evaluation Metrics for CoQA Model Performance
DESCRIPTION: Defines a function to calculate Exact Match (EM) and F1 metrics for evaluating model performance on the CoQA dataset. These metrics compare the model's predicted answers with ground truth answers to measure accuracy.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
# This function will be used to evaluate predicted answers uinsg the Exact Match (EM) and F1 metrics

def get_metrics(pred_answers):
    """
    Calculate the Exact Match (EM) and F1 metrics for predicted answers.
    Args:
        pred_answers (list): A list of predicted answers. Each element in the list is a list of predicted answers for a single question.
    Returns:
        tuple: A tuple containing two elements:
            - em_score (float): The average Exact Match score across all predictions.
            - f1_score (float): The average F1 score across all predictions.
    """

    em_metrics = []
    f1_metrics = []

    for pred, data in tqdm(zip(pred_answers, coqa_dataset["validation"]), total=len(pred_answers)):
        for pred_answer, true_answer in zip(pred, data["answers"]["input_text"]):
            em_metrics.append(squad_metrics.compute_exact(true_answer, pred_answer))
            f1_metrics.append(squad_metrics.compute_f1(true_answer, pred_answer))

    return sum(em_metrics) / len(em_metrics), sum(f1_metrics) / len(f1_metrics)
```

----------------------------------------

TITLE: Rendering the Knowledge Graph Visualization
DESCRIPTION: Calls the visualization function to create and render the knowledge graph as a PNG image.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
visualize_knowledge_graph(graph)
```

----------------------------------------

TITLE: Transforming CoQA Dataset to Chat Format for Fine-tuning
DESCRIPTION: Creates a mapping function that converts CoQA dataset rows into the required chat format for Together AI. The function structures each row as a conversation with a system prompt containing the story context, followed by alternating user questions and assistant answers.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# the system prompt,if present, must always be at the beginning
system_prompt = "Read the story and extract answers for the questions.\nStory: {}"

def map_fields(row):
    """    
    Maps the fields from a row of data to a structured format for conversation.
    Args:
        row (dict): A dictionary containing the keys "story", "questions", and "answers".
            - "story" (str): The story content to be used in the system prompt.
            - "questions" (list of str): A list of questions from the user.
            - "answers" (dict): A dictionary containing the key "input_text" which is a list of answers from the assistant.
    Returns:
        dict: A dictionary with a single key "messages" which is a list of message dictionaries.
            Each message dictionary contains:
            - "role" (str): The role of the message sender, either "system", "user", or "assistant".
            - "content" (str): The content of the message.    
    """
    # create system prompt
    messages = [{"role": "system", "content": system_prompt.format(row["story"])}]
    
    # add user and assistant messages
    for q, a in zip(row["questions"], row["answers"]["input_text"]):
        messages.append({"role": "user", "content": q})
        messages.append({"role": "assistant", "content": a})
    
    return {"messages": messages}
```

----------------------------------------

TITLE: Initiating DPO Fine-Tuning Job with Higher Beta
DESCRIPTION: Starts another DPO fine-tuning job with a higher DPO beta value (0.7) to demonstrate the effect of this parameter on preference tuning. This setting results in more conservative updates, staying closer to the reference model behavior.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
dpo_training = client.fine_tuning.create(
    training_file=dpo_train_file.id,
    validation_file=dpo_validation_file.id,
    n_evals=5,
    model=MODEL_NAME,
    wandb_api_key=WANDB_API_KEY,
    wandb_project_name="helpsteer2",
    suffix="helpsteer2_dpo_training",
    n_epochs=1,
    n_checkpoints=1,
    learning_rate=1e-5,
    lora=True,
    training_method='dpo',
    dpo_beta=0.7,  # HIGHER DPO_BETA
)
print(dpo_training.id)
```

----------------------------------------

TITLE: Defining Vector Retrieval Function in Python
DESCRIPTION: This function retrieves the top-k most similar items from a vector index based on a query string using cosine similarity. It requires an embedding generation step and numpy for handling arrays. The function outputs a list of indices of the most similar items.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_27

LANGUAGE: python
CODE:
```
def vector_retreival(query: str, top_k: int = 5, vector_index: np.ndarray = None) -> List[int]:
    """
    Retrieve the top-k most similar items from an index based on a query.
    Args:
        query (str): The query string to search for.
        top_k (int, optional): The number of top similar items to retrieve. Defaults to 5.
        index (np.ndarray, optional): The index array containing embeddings to search against. Defaults to None.
    Returns:
        List[int]: A list of indices corresponding to the top-k most similar items in the index.
    """

    query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]
    similarity_scores = cosine_similarity([query_embedding], vector_index)

    return list(np.argsort(-similarity_scores)[0][:top_k])
```

----------------------------------------

TITLE: Fine-Tuning Model - Python
DESCRIPTION: This snippet initiates the fine-tuning process for a specified model using multiple parameters such as training file id, epochs, batch size, and learning rate. The job id of the fine-tuning process is stored in 'task_fine_tuning_job_id'.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_22

LANGUAGE: python
CODE:
```
response = client.fine_tuning.create(
  training_file = task_file_id,
  model = 'meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference',
  n_epochs = 1,
  n_checkpoints = 1,
  batch_size = "max",
  learning_rate = 7e-5,
  suffix = 'long-context-finetune',
  wandb_api_key = WANDB_API_KEY,
  lora=True,
)
task_fine_tuning_job_id = response.id
```

----------------------------------------

TITLE: Generator Implementation
DESCRIPTION: Implementation of the Generator component that creates solutions based on tasks and feedback.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
def generate(task: str, generator_prompt: str, context: str = "") -> tuple[str, str]:
    """Generate and improve a solution based on feedback."""
    full_prompt = f"{generator_prompt}\n{context}\nTask: {task}" if context else f"{generator_prompt}\nTask: {task}"

    response = run_llm(full_prompt, model="Qwen/Qwen2.5-Coder-32B-Instruct")
    
    print("\n=== GENERATION START ===")
    print(f"Output:\n{response}\n")
    print("=== GENERATION END ===\n")
    
    return response
```

----------------------------------------

TITLE: Concatenating Retrieved Chunks into a Single String in Python
DESCRIPTION: This snippet creates a single string from the top documents returned by the reranker to be utilized in the final generative model's query for response generation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_41

LANGUAGE: python
CODE:
```
retreived_chunks = ''

for result in response.results:
    retreived_chunks += hybrid_top_k_docs[result.index] + '\n\n'

print(retreived_chunks)
```

----------------------------------------

TITLE: Filtering for High Quality Document Samples
DESCRIPTION: Extracts high-quality samples by filtering for documents with specific length ranges and high Wikipedia reference scores as a quality indicator.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
# Filter for high quality samples 

long_documents = []
for sample in tqdm(ds_iterator['train']):
    # From 64k tokens to 128k tokens
    if (len(sample['raw_content']) > 230000 and 
        len(sample['raw_content']) < 430000):
        
        signals = json.loads(sample["quality_signals"])
        try:
            wiki_score = signals['rps_doc_ml_wikiref_score'][0][-1]
        except:
            wiki_score = 0
            
        if (wiki_score > 0.5 and
            len(long_documents) < 2000):
            
            document = x['raw_content']
            long_documents.append(document)

            if len(long_documents) % 10 == 0:
                print(len(long_documents))

    if len(long_documents) >= 2000:
        break
```

----------------------------------------

TITLE: Writing Converted Dataset to JSONL Files
DESCRIPTION: Converts the dataset to the required format and writes the training and validation data to separate JSONL files. It also displays a sample example from the converted dataset.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
import json
import os

# Convert the dataset to the required format
converted_dataset = convert_to_preference_dataset(dataset)

# Create output directory if it doesn't exist
os.makedirs("data", exist_ok=True)

# Write training data
dpo_train_file_path = "data/helpsteer2_preference_train.jsonl"
with open(dpo_train_file_path, "w") as f:
    for example in converted_dataset["train"]:
        f.write(json.dumps(example) + "\n")

# Write validation data
dpo_validation_file_path = "data/helpsteer2_preference_validation.jsonl"
with open(dpo_validation_file_path, "w") as f:
    for example in converted_dataset["validation"]:
        f.write(json.dumps(example) + "\n")

print(f"Saved {len(converted_dataset['train'])} training examples to data/helpsteer_preference.jsonl")
print(f"Validation set contains {len(converted_dataset['validation'])} examples")

# Display a sample example
print("\nSample example:")
print(json.dumps(converted_dataset["train"][0], indent=2))
```

----------------------------------------

TITLE: Implementing Chunked BERTScore Evaluation
DESCRIPTION: Defines a function that calculates BERTScore between chunks of the original document and the generated summary to evaluate how well each section is represented in the summary.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
from bert_score import score

def chunked_BERTscores(reference: str, candidate: str):
    """
    This function calculates the BERTScore for a candidate summary against chunks of a reference text.
     reference: text of the original document
     candidate: LLM generated summary of the original document
    """
    ref_chunks = [reference[i - 400 : i] for i in range(400,len(reference),400)]
    cand =  [candidate]*len(ref_chunks)

    P, R, F1 = score(ref_chunks, cand, lang="en", model_type="facebook/bart-large-mnli") #instead of using the original BERT model we use the improved BART model

    return (P, R, F1)
```

----------------------------------------

TITLE: Continual Fine-tuning on Additional Function Calling Categories
DESCRIPTION: Continues fine-tuning from the previous checkpoint using the second dataset, demonstrating the core concept of Continual Fine-tuning.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
ft_fc2 = client.fine_tuning.create(
    training_file = resp_fc2.id, # function_calling_2.jsonl
    from_checkpoint = ft_fc1.id, # "id" from the first run above 
    n_epochs = 1,
    n_checkpoints = 1,
    batch_size = 16,
    lora=True,
    warmup_ratio=0.1,
    wandb_api_key=WANDB_API_KEY,
)
print(ft_fc2.id)
```

----------------------------------------

TITLE: Creating an SFT Fine-tuning Job on Together AI
DESCRIPTION: Creates a Supervised Fine-Tuning (SFT) job on the Together AI platform. This is the first stage of the optional two-stage approach, configuring parameters like learning rate, epochs, and enabling LoRA for efficient training.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
# If you want to combine the SFT + DPO training, you can do so by creating a SFT job first
# and then using the DPO training to continue the training of the resulting SFT checkpoint.

sft_training = client.fine_tuning.create(
    training_file=sft_train_file.id,
    validation_file=sft_validation_file.id,
    n_evals=3,
    model=MODEL_NAME,
    wandb_api_key=WANDB_API_KEY,
    wandb_project_name="helpsteer2",
    suffix="helpsteer2_sft_training",
    n_epochs=1,
    n_checkpoints=1,
    learning_rate=1e-5,
    lora=True,
)
print(sft_training.id)
```

----------------------------------------

TITLE: Creating LoRA Finetuning Job
DESCRIPTION: Configures and initiates a LoRA finetuning job on the Llama-3.2-1B-Instruct model. The parameters specify training settings like epochs, learning rate, and warmup ratio, with LoRA enabled for efficient training.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
ft_resp = client.fine_tuning.create(
    training_file = train_file_resp.id,
    model = 'meta-llama/Llama-3.2-1B-Instruct', # changed to 1B model
    train_on_inputs= "auto",
    n_epochs = 3,
    n_checkpoints = 1,
    wandb_api_key = WANDB_API_KEY,
    lora = True,
    warmup_ratio=0,
    learning_rate = 1e-5,
    suffix = 'FT-webinar-demo-1b',
)

print(ft_resp.id)
```

----------------------------------------

TITLE: Executing Email Send with Processed Tool
DESCRIPTION: This snippet demonstrates how to execute the email send with the processed tool and handle the response. It creates a chat completion using the Together AI client with the processed email tool and then handles the tool calls to send the email.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# Execute chat completion with processed tool
response = client.chat.completions.create(
    model="Qwen/Qwen2.5-72B-Instruct-Turbo",
    messages=[
        {"role": "system", "content": "You are Alex, a product manager at an AI company."},
        {
            "role": "user",
            "content": "Send an email to John, who is an ML engineer on the team, to inquire about a good time to meet next week to review the upcoming launch.",
        },
    ],
    tools=processed_send_email_tool,
)

# Handle the tool calls and send the email
exec_response = toolset.handle_tool_calls(response)
```

----------------------------------------

TITLE: Preparing Movie Data for Embedding
DESCRIPTION: Concatenates the title, overview, and tagline of each movie to create a rich text representation for embedding, processing the first 1000 movies from the dataset.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# Concatenate the title, overview, and tagline of each movie
# this makes the text that will be embedded for each movie more informative
# as a result the embeddings will be richer and capture this information. 
to_embed = []
for movie in movies_data[:1000]:
    text = ''
    for field in ['title', 'overview', 'tagline']:
        value = movie.get(field, '')
        text += str(value) + ' '
    to_embed.append(text.strip())

to_embed[:10]
```

----------------------------------------

TITLE: Retrieving Top 5 Indices by Similarity
DESCRIPTION: Gets the indices of the top 5 most similar chunks based on similarity scores.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
top_5_indices = indices[:5]
top_5_indices
```

----------------------------------------

TITLE: Testing Vector Database Search Functionality
DESCRIPTION: This code demonstrates how to perform a similarity search using the vector database. It searches for documents related to a given query and prints the results.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# example search from vectorDB
query = "How do I jailbreak a LLM?"
docs = vectorstore.similarity_search(query, k=3)
for i in [docs[j].page_content+'\n\n' for j in range(len(docs))]:
    print(i)
    print('-'*100)
```

----------------------------------------

TITLE: Example JSON Format for Conversational Fine-tuning Data
DESCRIPTION: Shows the required JSON format for conversational data used in Together AI fine-tuning. Each example contains a list of messages with roles (system, user, assistant) and content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_1

LANGUAGE: json
CODE:
```
{"messages": [{"role": "system", "content": "You are a helpful assistant."}, 
              {"role": "user", "content": "Hello!"}, 
              {"role": "assistant", "content": "Hi! How can I help you?"}]}
```

----------------------------------------

TITLE: Transforming CoQA Dataset to Chat Format
DESCRIPTION: Applies the mapping function to transform the CoQA dataset into a structured conversational format suitable for finetuning LLMs. The transformation maintains the question-answer pairs in a chat-like structure.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# transform the data using the mapping function
train_messages = coqa_dataset["train"].map(map_fields, remove_columns=coqa_dataset["train"].column_names)
```

----------------------------------------

TITLE: Scraping Essay Content in Python
DESCRIPTION: Actor task that scrapes the content of each essay URL with retry functionality for error handling.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
@actor.task(retries=3)
def scrape_pg_essays(document: Document) -> Document:
    from bs4 import BeautifulSoup

    try:
        response = requests.get(document.url)
    except Exception as e:
        raise FlyteRecoverableException(f"Failed to scrape {document.url}: {str(e)}")
    
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    content = soup.find("font")

    text = None
    if content:
        text = " ".join(content.get_text().split())
    document.content = text
    return document
```

----------------------------------------

TITLE: Workflow Example Usage
DESCRIPTION: Example implementation of using the looping workflow to solve a stack implementation problem.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
task = """
Implement a Stack with:
1. push(x)
2. pop()
3. getMin()
All operations should be O(1).
"""

loop_workflow(task, EVALUATOR_PROMPT, GENERATOR_PROMPT)
```

----------------------------------------

TITLE: Computing Semantic Similarity Between Query and Movies
DESCRIPTION: Calculates cosine similarity scores between the query embedding and all movie embeddings to determine semantic relevance.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
from sklearn.metrics.pairwise import cosine_similarity

# This function will produce a list of cosine similarity scores between the query embedding and the movie embeddings
similarity_scores = cosine_similarity([query_embedding], embeddings)
```

----------------------------------------

TITLE: Calling Iterative Research Function in Python
DESCRIPTION: This code snippet demonstrates how to call the `conduct_iterative_research` function. It passes the research topic, initial results, initial queries, budget, maximum queries, Tavily client, Together AI client, planning model, JSON model, summary model, and prompts to the function. The function returns the final results and all queries used.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
results, all_queries=await conduct_iterative_research(topic=research_topic, initial_results=initial_results, all_queries=initial_queries, budget=budget, max_queries=max_queries, tavily_client=tavily_client, together_client=together_client, planning_model=planning_model, json_model=json_model, summary_model=summary_model, prompts=prompts)
```

----------------------------------------

TITLE: Testing Agent with Stream and pprint
DESCRIPTION: This code snippet tests the agent by providing a user query as input. It then streams the output from the graph and iterates through each node's output. The `pprint` module is used to display the output from each node in a structured and readable format.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
import pprint

inputs = {
    "messages": [
        ("user", "What are the different types of agent memory?"),
    ]
}
for output in graph.stream(inputs):
    for key, value in output.items():
        pprint.pprint(f"Output from node '{key}':")
        pprint.pprint("---")
        pprint.pprint(value, indent=2, width=80, depth=None)
    pprint.pprint("\n---\n")
```

----------------------------------------

TITLE: Initializing ColPali Model for MultiModal RAG
DESCRIPTION: Imports necessary modules and initializes the RAGMultiModalModel using the ColQwen2 pretrained model for document indexing and retrieval.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
import os
from pathlib import Path
from byaldi import RAGMultiModalModel

# Initialize RAGMultiModalModel
model = RAGMultiModalModel.from_pretrained("vidore/colqwen2-v0.1")
```

----------------------------------------

TITLE: Generating Embeddings for Contextual Chunks
DESCRIPTION: Applies the embedding generation function to create vector embeddings for all contextual chunks using the BGE-Large model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_20

LANGUAGE: python
CODE:
```
contextual_embeddings = generate_embeddings(contextual_chunks, "BAAI/bge-large-en-v1.5")
```

----------------------------------------

TITLE: Displaying Color-Coded Text Based on BERTScores
DESCRIPTION: Scales the F1 BERTScores and applies the color-coding visualization to show which parts of the original document are well-represented (green) or poorly represented (red) in the summary.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
data_array = [[x] for x in scores[2].tolist()]

# Initialize MinMaxScaler
scaler = MinMaxScaler() # we just use this to make the colours more identifiable

# Fit and transform the data
scaled_data = scaler.fit_transform(data_array)

# Convert the scaled data back to a list
scaled_F1 = [x[0] for x in scaled_data]

print_text_in_colors(text,scaled_F1)
```

----------------------------------------

TITLE: Creating Prompts for All Chunks
DESCRIPTION: Applies the generate_prompts function to create a prompt for each chunk in the document.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
prompts = generate_prompts(pg_essay, chunks)
```

----------------------------------------

TITLE: Generating Embeddings for Document Chunks
DESCRIPTION: Applies the embedding generation function to all document chunks using the BGE large English model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
embeddings = generate_embeddings(list(chunks), "BAAI/bge-large-en-v1.5")
```

----------------------------------------

TITLE: Defining Planning Structure Types
DESCRIPTION: Creates type definitions to manage the planning state, including PlanExecute and Plan classes.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
import operator
from typing import Annotated, List, Tuple
from typing_extensions import TypedDict

# Define types for the planning system
class PlanExecute(TypedDict):
    input: str
    plan: List[str]
    past_steps: Annotated[List[Tuple], operator.add]
    response: str
```

LANGUAGE: python
CODE:
```
from pydantic import BaseModel, Field


class Plan(BaseModel):
    """Plan to follow in future"""

    steps: List[str] = Field(
        description="different steps to follow, should be in sorted order"
    )
```

----------------------------------------

TITLE: Uploading Dataset for LoRA Finetuning
DESCRIPTION: Uploads a JSONL dataset file to Together AI's platform. This dataset will be used for finetuning the model with the LoRA technique.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LoRA_Finetuning&Inference.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Upload dataset to Together AI

train_file_resp = client.files.upload("datasets/small_coqa_10.jsonl", check=True)
print(train_file_resp.id)
```

----------------------------------------

TITLE: Implementing Embedding Generation Function
DESCRIPTION: Defines a function to generate embeddings from input texts using the Together AI API. Takes a list of texts and model identifier and returns the vector embeddings.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
from typing import List

def generate_embeddings(input_texts: List[str], model_api_string: str) -> List[List[float]]:
    """Generate embeddings from Together python library.

    Args:
        input_texts: a list of string input texts.
        model_api_string: str. An API string for a specific embedding model of your choice.

    Returns:
        embeddings_list: a list of embeddings. Each element corresponds to the each input text.
    """
    together_client = together.Together(api_key = TOGETHER_API_KEY)
    outputs = together_client.embeddings.create(
        input=input_texts,
        model=model_api_string,
    )
    return [x.embedding for x in outputs.data]
```

----------------------------------------

TITLE: Displaying Comprehensive Search Results in Python
DESCRIPTION: This code displays an overview of the combined search results. It shows the first and last portions of the result set to provide a glimpse of the information gathered, demonstrating how multiple sources are consolidated into a single research corpus.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
print(f"First 10000 characters of {len(initial_results.results)} results:\n\n {str(initial_results)[:10000]}\n\n...\n\n Last 10000 characters of {len(initial_results.results)} results:\n\n {str(initial_results)[-10000:]}\n\n")
```

----------------------------------------

TITLE: Generating Image with Multiple LoRAs
DESCRIPTION: Demonstrates using multiple LoRA fine-tunes together, combining a tarot card style with a detail-enhancing LoRA. It shows how to adjust the scale of each LoRA for desired effects.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
prompt = "a baby panda eating bamboo in the style of TOK a trtcrd tarot style"

generated_image = generate_image(prompt, 
                                 lora1="https://huggingface.co/multimodalart/flux-tarot-v1",
                                 scale1=1.0,
                                 lora2="https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-add-details",
                                 scale2=0.8
                                 )

Image(url=generated_image, width=512, height=384)
```

----------------------------------------

TITLE: Uploading Training Files to Together AI Platform
DESCRIPTION: Uploads the prepared JSONL files to the Together AI platform for processing and use in fine-tuning jobs.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
resp_fc1 = client.files.upload(file="function_calling_1.jsonl", check=True)
print(resp_fc1.id)
```

LANGUAGE: python
CODE:
```
resp_fc2 = client.files.upload(file="function_calling_2.jsonl", check=True) # uploads a file
print(resp_fc2.id)
```

----------------------------------------

TITLE: Defining Prediction Generation Function for Summarization in Python
DESCRIPTION: This function generates predictions for a list of test items using a specified model. It utilizes the llm_call function to interact with the model and returns a list of predictions.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
def get_predictions(test_items, model):
    predictions = []
    for it in tqdm(test_items):
        predictions.append(
            llm_call(it['prompt'],
                     model=model
                    )
        )
    return predictions
```

----------------------------------------

TITLE: Uploading SFT Datasets to Together AI for Fine-tuning
DESCRIPTION: Uploads the prepared SFT datasets to Together AI cloud storage for use in fine-tuning. The code sets check=True to trigger format validation, ensuring the data is properly formatted for SFT.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
sft_train_file = client.files.upload(sft_train_output_path, check=True)
sft_validation_file = client.files.upload(sft_validation_output_path, check=True)

print(f"Uploaded SFT training files: {sft_train_file}")
print(f"Uploaded SFT validation files: {sft_validation_file}")


```

----------------------------------------

TITLE: Model Output Schema Definition - Pydantic Model
DESCRIPTION: Defines the schema for model selection output using Pydantic, including available models and reasoning field
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
from pydantic import BaseModel, Field
from typing import Literal

class ModelOutput(BaseModel):
    model: Literal["deepseek-ai/DeepSeek-V3",
                   "Qwen/Qwen2.5-Coder-32B-Instruct", 
                   "Gryphe/MythoMax-L2-13b", 
                   "Qwen/QwQ-32B-Preview",
                   "meta-llama/Llama-3.3-70B-Instruct-Turbo"]
    
    reason: str = Field(
        description="Reason why this model was selected for the task specified in the prompt/query."
    )
```

----------------------------------------

TITLE: Generating Image with LLM
DESCRIPTION: This code snippet demonstrates how to use an LLM to generate an image based on a provided request. It uses the `client.chat.completions.create` method to interact with the LLM, providing a model, messages, and tools. The generated image is then processed using the `th.run_tools` function.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
image_response = client.chat.completions.create(
  model=MODEL,
  messages=generate_image_request,
  tools=tools,
)
tool_run = th.run_tools(image_response)
```

----------------------------------------

TITLE: Embedding User Query
DESCRIPTION: Generates and visualizes the embedding vector for a user query about "super hero action movie with a timeline twist" using the same embedding model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
query = "super hero action movie with a timeline twist"

query_embedding = generate_embeddings([query], 'BAAI/bge-base-en-v1.5')[0]

data_2d = np.reshape(query_embedding, (1, 768))

plt.figure(figsize=(15, 1))
# Create a heatmap with a binary colormap (black and white)
plt.imshow(data_2d, cmap='binary', interpolation='nearest', aspect='auto')

# Remove axes and ticks
plt.xlabel('Vector Dimension')
plt.ylabel("query")
# Show the plot
plt.show()
```

----------------------------------------

TITLE: Defining Sample Flight Data and Usage Limits
DESCRIPTION: This code block provides sample flight data as a string and sets usage limits to prevent excessive API calls during development and testing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# Sample flight data with various origin/destination/date combinations
flights_web_page = """
1. Flight SFO-AK123
- Price: $350
- Origin: San Francisco International Airport (SFO)
- Destination: Ted Stevens Anchorage International Airport (ANC)
- Date: January 10, 2025

2. Flight SFO-AK456
- Price: $370
- Origin: San Francisco International Airport (SFO)
- Destination: Fairbanks International Airport (FAI)
- Date: January 10, 2025

3. Flight SFO-AK789
- Price: $400
- Origin: San Francisco International Airport (SFO)
- Destination: Juneau International Airport (JNU)
- Date: January 20, 2025

4. Flight NYC-LA101
- Price: $250
- Origin: San Francisco International Airport (SFO)
- Destination: Ted Stevens Anchorage International Airport (ANC)
- Date: January 10, 2025

5. Flight CHI-MIA202
- Price: $200
- Origin: Chicago O'Hare International Airport (ORD)
- Destination: Miami International Airport (MIA)
- Date: January 12, 2025

6. Flight BOS-SEA303
- Price: $120
- Origin: Boston Logan International Airport (BOS)
- Destination: Ted Stevens Anchorage International Airport (ANC)
- Date: January 12, 2025

7. Flight DFW-DEN404
- Price: $150
- Origin: Dallas/Fort Worth International Airport (DFW)
- Destination: Denver International Airport (DEN)
- Date: January 10, 2025

8. Flight ATL-HOU505
- Price: $180
- Origin: Hartsfield-Jackson Atlanta International Airport (ATL)
- Destination: George Bush Intercontinental Airport (IAH)
- Date: January 10, 2025
"""

# We set usage limits to prevent excessive API calls during development/testing
usage_limits = UsageLimits(request_limit=15)
```

----------------------------------------

TITLE: Performing Text-to-Image Retrieval Using JinaCLIP in Python
DESCRIPTION: Demonstrates text-to-image retrieval by searching for the most semantically similar image to a given text query using the retrieve_image function.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
retrieved_image = links[retrieve_image(query = 'family pics', query_type = 'text', index = image_embeddings)]
```

----------------------------------------

TITLE: Testing Model with Irrelevant Query
DESCRIPTION: Tests how the DeepSeek R1 model handles a query that's irrelevant to the provided context, demonstrating its reasoning capabilities when information is missing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_17

LANGUAGE: python
CODE:
```
query = "What is the circumference of the moon?"

stream = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1",
    messages=[
      {"role": "system", "content": "You are a helpful chatbot."},
      {"role": "user", "content": PROMPT.format(query=query, formatted_chunks=formatted_chunks)},
    ],
      stream=True,
)

response = ''

for chunk in stream:
  response += chunk.choices[0].delta.content or ""
  print(chunk.choices[0].delta.content or "", end="", flush=True)
```

----------------------------------------

TITLE: Generating Context for All Chunks
DESCRIPTION: Creates a list of contextual chunks by generating context for each chunk and concatenating it with the original chunk content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_17

LANGUAGE: python
CODE:
```
# Let's generate the entire list of contextual chunks

contextual_chunks = [generate_context(prompts[i])+' '+chunks[i] for i in range(len(chunks))]
```

----------------------------------------

TITLE: Initiating LLM Finetuning on CoQA Dataset
DESCRIPTION: Creates a finetuning job for Llama 3.1 8B using the uploaded CoQA dataset. This configures the finetuning parameters including learning rate, epochs, and LoRA adaptation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
ft_resp = client.fine_tuning.create(
    training_file = train_file_resp.id,
    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference',
    train_on_inputs= "auto",
    n_epochs = 3,
    n_checkpoints = 1,
    wandb_api_key = WANDB_API_KEY,
    lora = True,
    warmup_ratio=0,
    learning_rate = 1e-5,
    suffix = 'my-demo-finetune',
)

print(ft_resp.id)
```

----------------------------------------

TITLE: Loading and Visualizing Function Calling Dataset Categories
DESCRIPTION: Loads the NousResearch's Hermes Function Calling dataset, counts categories, and creates a bar chart of the top 10 categories.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
dataset_name = "NousResearch/hermes-function-calling-v1"
function_calling = load_dataset(dataset_name)

# Count categories
category_counts = Counter(function_calling["train"]["category"])

# Get top 10 categories
top_categories = category_counts.most_common(10)
categories, counts = zip(*top_categories)

# Create bar chart
plt.figure(figsize=(12, 6))
plt.bar(categories, counts)
plt.xticks(rotation=45, ha='right')
plt.title('Top 10 Categories in Function Calling Dataset')
plt.xlabel('Category')
plt.ylabel('Count')
plt.tight_layout()
plt.show()
```

----------------------------------------

TITLE: Retrieving Fine-tuned Model Prediction Example in Python
DESCRIPTION: This code extracts a specific prediction (index 20) from the fine-tuned model's results on the GovReport dataset for comparative analysis with the baseline model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
predictions_ft_govreport[20]
```

----------------------------------------

TITLE: Fetching Legislative Text from LegiScan
DESCRIPTION: Downloads the text content of California Senate Bill SB1047 from LegiScan using requests and BeautifulSoup to parse the HTML.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import requests
from bs4 import BeautifulSoup

def get_legiscan_text(url):
    """
    Fetches and returns the text content from a given LegiScan URL.
    Args:
        url (str): The URL of the LegiScan page to fetch.
    Returns:
        str: The text content of the page.
    Raises:
        requests.exceptions.RequestException: If there is an issue with the HTTP request.
    """
    # Basic headers to mimic a browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    # Make the request
    response = requests.get(url, headers=headers)

    # Parse HTML
    soup = BeautifulSoup(response.text, 'html.parser')

    # Get text content
    content = soup.get_text()

    return content

url = "https://legiscan.com/CA/text/SB1047/id/2999979/California-2023-SB1047-Amended.html"
text = get_legiscan_text(url)
print(text)
```

----------------------------------------

TITLE: Printing Retrieved Documents from BM25 Results in Python
DESCRIPTION: This snippet iterates over the retrieved results and prints the index and document for each chunk, allowing the user to see which documents were deemed most relevant according to the BM25 search.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_31

LANGUAGE: python
CODE:
```
for doc in results[0]:
  print(f"Chunk Index {contextual_chunks.index(doc)} : {doc}")
```

----------------------------------------

TITLE: Implementing Synchronous LLM Helper Function
DESCRIPTION: Creates a helper function that handles the basic interaction with language models through the Together API, supporting both user and system prompts.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Simple LLM call helper function
def run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):
    """ Run the language model with the given user prompt and system prompt. """
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": user_prompt})
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.7,
        max_tokens=4000,        
    )

    return response.choices[0].message.content
```

----------------------------------------

TITLE: Initializing Together AI and Composio for Email Sending
DESCRIPTION: This snippet sets up the integration between Together AI's LLM and Composio's Gmail tool. It initializes the Together AI client, creates a Composio toolset, configures email sending capability, and makes an LLM call with the email tool to test the setup.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
import os
from composio_togetherai import ComposioToolSet, Action
from together import Together

# Initialize Together AI and Composio clients with API keys
client = Together(api_key=os.getenv("TOGETHER_API_KEY"))
toolset = ComposioToolSet(api_key=os.getenv("COMPOSIO_API_KEY"))

# Get the Gmail send email tool
send_email_tool = toolset.get_tools(
    [Action.GMAIL_SEND_EMAIL], check_connected_accounts=False
)

# Create a chat completion with email capability
response = client.chat.completions.create(
    model="Qwen/Qwen2.5-72B-Instruct-Turbo",
    messages=[
        {"role": "system", "content": "You are Alex, a product manager at an AI company."},
        {
            "role": "user",
            "content": "Send an email to John, who is an ML engineer on the team, to inquire about a good time to meet next week to review the upcoming launch.",
        },
    ],
    tools=send_email_tool,
)

response.model_dump()
```

----------------------------------------

TITLE: Registering a Scheduled Launch Plan
DESCRIPTION: This code defines and registers a launch plan that schedules the `build_indices_wf` workflow to run daily at 1:00 AM. It uses `fl.LaunchPlan.get_or_create` to either retrieve an existing launch plan or create a new one if it doesn't exist. The `auto_activate=True` argument ensures the launch plan is automatically activated upon registration.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
lp = fl.LaunchPlan.get_or_create(
    workflow=build_indices_wf,
    name="vector_db_ingestion_activate",
    schedule=fl.CronSchedule(
        schedule="0 1 * * *"
    ),  # Run every day to update the databases
    auto_activate=True,
)

registered_lp = remote.register_launch_plan(entity=lp)
```

----------------------------------------

TITLE: Launching Fine-tuning Job with Together AI
DESCRIPTION: Creates and initiates a fine-tuning job using the Together AI API. Configures important parameters like model selection, training file, number of epochs, learning rate, and LoRA adaptation settings.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
## This fine-tuning job should take ~10-15 minutes to complete

ft_resp = client.fine_tuning.create(
    training_file = "file-19c6ef51-b734-4f3c-bc17-62fbad2bd0d0",
    model = 'meta-llama/Meta-Llama-3.1-8B-Instruct-Reference',
    train_on_inputs= "auto",
    n_epochs = 3,
    n_checkpoints = 1,
    wandb_api_key = WANDB_API_KEY,
    lora = True,
    warmup_ratio=0,
    learning_rate = 1e-5,
    suffix = 'test1_8b',
)

print(ft_resp.id)
```

----------------------------------------

TITLE: Defining Conversation Format Mapping Function for CoQA
DESCRIPTION: Creates a function to transform CoQA dataset rows into the conversational format required for finetuning. The function structures data into system, user, and assistant messages with appropriate content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# the system prompt,if present, must always be at the beginning
system_prompt = "Read the story and extract answers for the questions.\nStory: {}"

def map_fields(row):
    """    
    Maps the fields from a row of data to a structured format for conversation.
    Args:
        row (dict): A dictionary containing the keys "story", "questions", and "answers".
            - "story" (str): The story content to be used in the system prompt.
            - "questions" (list of str): A list of questions from the user.
            - "answers" (dict): A dictionary containing the key "input_text" which is a list of answers from the assistant.
    Returns:
        dict: A dictionary with a single key "messages" which is a list of message dictionaries.
            Each message dictionary contains:
            - "role" (str): The role of the message sender, either "system", "user", or "assistant".
            - "content" (str): The content of the message.    
    """
    messages = [
        {
            "role": "system",
            "content": system_prompt.format(row["story"]),
        }
    ]
    for q, a in zip(row["questions"], row["answers"]["input_text"]):
        messages.append(
            {
                "role": "user",
                "content": q,
            }
        )
        messages.append(
            {
                "role": "assistant",
                "content": a,
            }
        )

    return {
        "messages": messages
    }
```

----------------------------------------

TITLE: Calculating Cosine Similarity and Sorting Indices in Python
DESCRIPTION: This snippet calculates the cosine similarity between a query embedding and contextual embeddings, then retrieves the top indices based on similarity scores. It depends on the numpy library for operations on arrays.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_24

LANGUAGE: python
CODE:
```
similarity_scores = cosine_similarity([query_embedding], contextual_embeddings)
indices = np.argsort(-similarity_scores)
```

----------------------------------------

TITLE: Loading the FineWeb Educational Dataset
DESCRIPTION: Loads a sample dataset from the FineWeb educational corpus to create long context examples for testing and fine-tuning.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
from datasets import load_dataset
ds_iterator = load_dataset(
    "HuggingFaceFW/fineweb-edu",
    "sample-10BT",
)['train']
```

----------------------------------------

TITLE: Extracting F1 Scores from BERTScore Results
DESCRIPTION: Accesses the F1 component of the BERTScore results, which is a combined measure of precision and recall, to use as the main evaluation metric.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
# Each BERTScore below ranges from 0 to 1, 1 being the best score
# BERTScore returns Precision, Recall, and F1 score - we will simply use the F1 score as our evaluation metric
scores[2]
```

----------------------------------------

TITLE: Creating Repetition Task Dataset
DESCRIPTION: Creates a task dataset where each example asks the model to return the last N words from a document, where N is randomly selected between 1 and 100.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
long_documents = orjson.loads(Path("long_documents.json").read_bytes())
long_documents_32k = long_documents["32k"]

task_items = []

for document in long_documents_32k:
    n = np.random.randint(1, 100)
    prompt = f"Return last {n} words from this text: \n\n"
    target = " ".join(document.split()[-n:])

    task_items.append({
        "prompt": prompt + document,
        "completion": target
    })
```

----------------------------------------

TITLE: Visualizing LangGraph Workflow with Mermaid
DESCRIPTION: This code snippet uses the IPython.display module to display the LangGraph workflow as a Mermaid diagram. It calls the `get_graph()` method of the compiled graph object, specifies that X-ray mode is enabled, and then renders the Mermaid diagram as a PNG image.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/Agentic_RAG_LangGraph.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
from IPython.display import Image, display


display(Image(graph.get_graph(xray=True).draw_mermaid_png()))

```

----------------------------------------

TITLE: Setting Up Evaluation Metrics for the Agent
DESCRIPTION: Creates an evaluation metric (top5_recall) to measure the agent's performance by comparing its output against gold standard Wikipedia titles needed for fact verification.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
def top5_recall(example, pred, trace=None):
    gold_titles = example.titles
    recall = sum(x in pred.titles[:5] for x in gold_titles) / len(gold_titles)

    # If we're "bootstrapping" for optimization, return True if and only if the recall is perfect.
    if trace is not None:
        return recall >= 1.0
    
    # If we're just doing inference, just measure the recall.
    return recall

evaluate = dspy.Evaluate(devset=devset, metric=top5_recall, num_threads=16, display_progress=True, display_table=5)
```

----------------------------------------

TITLE: Launching Fine-tuning Jobs for Summarization in Python
DESCRIPTION: This code demonstrates how to launch fine-tuning jobs for both synthetic and GovReport datasets using the previously defined send_ft_job function. It sets specific parameters for each dataset, such as learning rate and number of epochs.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# Example fine-tuning job on the synthetic summarization dataset

summarization_fine_tuning_job_id = send_ft_job(
    client,
    model="meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference",
    learning_rate=7e-5,
    n_epochs=4,
    filename="1112_summarization_train.jsonl",
    run_name="1112-summarization-synthetic-no-loss-masking",
    summarization_file_id=None,
    train_on_inputs=True,
)

# Example fine-tuning job on GovReport dataset

summarization_fine_tuning_job_id = send_ft_job(
    client,
    model="meta-llama/Meta-Llama-3.1-8B-32k-Instruct-Reference",
    n_epochs=2,
    filename="1114_govreport.jsonl",
    run_name="1114-govreport-2ep",
    summarization_file_id=None,
    train_on_inputs=False,
    learning_rate=4e-5,
)
```

----------------------------------------

TITLE: Defining Model Answer Generation Function for CoQA Evaluation
DESCRIPTION: Creates a function to generate model answers for the CoQA validation set. This function parallels the answer generation process using a thread pool for efficiency when evaluating model performance.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
from tqdm.auto import tqdm
from multiprocessing.pool import ThreadPool
import transformers.data.metrics.squad_metrics as squad_metrics
```

----------------------------------------

TITLE: Extracting Top 5 Chunks Based on Indices in Python
DESCRIPTION: This snippet extracts the top 5 chunks from a list of contextual chunks using the top indices calculated from the cosine similarity scores.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_26

LANGUAGE: python
CODE:
```
top_5_chunks = [contextual_chunks[index] for index in indices[0]][:5]
top_5_chunks
```

----------------------------------------

TITLE: Initializing LLM with Together AI's Llama Model
DESCRIPTION: Sets up the Language Model using Together AI's Llama model as the core reasoning engine. It requires an API key from the environment variables.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/LangGraph/LangGraph_Planning_Agent.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import os
from langchain_together import ChatTogether

llm = ChatTogether(model="meta-llama/Llama-3.3-70B-Instruct-Turbo", api_key=os.environ.get("TOGETHER_API_KEY"))
```

----------------------------------------

TITLE: Testing LLM Performance on Sample Task
DESCRIPTION: Tests how a large language model performs on a single example of the repetition task by sending the prompt to the API.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
# How does a LLM model perform on this task item?

query = item['prompt']

result = llm_call(query)
```

----------------------------------------

TITLE: Displaying the Synthesized Final Answer
DESCRIPTION: Prints the final synthesized answer that combines the insights from all worker LLMs into a comprehensive solution to the original task.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
print(final_answer)
```

----------------------------------------

TITLE: Running Workflow and Retrieval Task Locally
DESCRIPTION: This code snippet demonstrates how to execute the `build_indices_wf` workflow and the `retrieve` task locally. It first calls the workflow to build the indices and then calls the retrieval task with the generated indices. Finally, it prints the results of the retrieval.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
if __name__ == "__main__":
    bm25s_index, contextual_chunks_data = build_indices_wf()
    results = retrieve(
        bm25s_index=bm25s_index, contextual_chunks_data=contextual_chunks_data
    )
    print(results)
```

----------------------------------------

TITLE: Formatting Dataset for Together AI Fine-tuning
DESCRIPTION: Converts the dataset into Together AI's required JSONL format with proper message roles (system, user, assistant) for both training steps.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
map_from = {"system": "system", "human": "user", "gpt": "assistant"}

with open("function_calling_1.jsonl", "w") as pfile:
    for line in first_step_train:
        conversations = line["conversations"]
        pfile.write(json.dumps(
            {"messages": [{"role": map_from[m["from"]], "content": m["value"] } for m in conversations]}
        ) + "\n")


with open("function_calling_2.jsonl", "w") as pfile:
    for line in second_step_train:
        conversations = line["conversations"]
        pfile.write(json.dumps(
            {"messages": [{"role": map_from[m["from"]], "content": m["value"] } for m in conversations]}
        ) + "\n")
```

----------------------------------------

TITLE: Creating or Updating Applications Remotely with Union in Python
DESCRIPTION: This snippet demonstrates how to create or update FastAPI and Gradio applications remotely using the Union framework's AppRemote. The 'AppRemote' object connects to the Union service and performs the operations on default development domain. It ensures the applications are up-to-date, simplifying application lifecycle management. This code relies on the prior setup of application objects and their specifications.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_15

LANGUAGE: python
CODE:
```
from union.remote._app_remote import AppRemote

app_remote = AppRemote(project="default", domain="development")

app_remote.create_or_update(fastapi_app)
app_remote.create_or_update(gradio_app)
```

----------------------------------------

TITLE: Plotting BERTScores by Document Position
DESCRIPTION: Creates a line plot showing how BERTScores vary across different positions in the document, using a smoothed version of F1 scores to improve readability.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
import numpy as np
import matplotlib.pyplot as plt


# We smoothen the F1 scores to make the plot more readable
def rolling_window(data, window_size):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

smoothed_F1 = rolling_window(scores[2], window_size=3)

plt.plot(np.linspace(0, 1, len(smoothed_F1)), smoothed_F1)
plt.xlabel('Position in the Document')
plt.ylabel('Score - higher is better')
plt.title('Summary Score')
plt.show()
```

----------------------------------------

TITLE: Testing the Optimized Agent on a Complex Claim
DESCRIPTION: Tests the optimized agent on a complex claim about playwrights to see the improved results after MIPRO optimization.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
optimized_react(claim="The author of the 1960s unproduced script written for The Beatles, Up Against It, and Bernard-Marie Kolts are both playwrights.").titles
```

----------------------------------------

TITLE: Example Chat Message Format for Conversational Finetuning
DESCRIPTION: Provides an example of the JSON format used for conversational finetuning. This format structures the conversation with system, user, and assistant messages in a sequential flow.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_13

LANGUAGE: json
CODE:
```
{
  "messages": [
    {"role": "system", "content": "You are a helpful AI chatbot."},
    {"role": "user", "content": "Hello, how are you?"},
    {"role": "assistant", "content": "I'm doing well, thank you! How can I help you?"},
    {"role": "user", "content": "Can you explain machine learning?"},
    {"role": "assistant", "content": "Machine learning is..."}
  ]
}
```

----------------------------------------

TITLE: Calculating Context Generation Costs
DESCRIPTION: Estimates the cost of generating contextual chunks based on token count and model pricing, and calculates how many chunks can be processed with a $1.00 budget.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
print(f"Cost to generate contextual chunks = ${32*0.06*(1660/1000000)}")
print(f"Number of Chunks Processed with $1.00 = {1000000/(0.06*1660)} chunks")
```

----------------------------------------

TITLE: Defining Knowledge Graph Schema with Pydantic
DESCRIPTION: Creates Pydantic classes to define the structure of a knowledge graph, including Node and Edge classes, which will be used to guide the LLM in generating properly structured JSON output.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
from pydantic import BaseModel, Field
from typing import List

class Node(BaseModel, frozen=True):
    id: int
    label: str

class Edge(BaseModel, frozen=True):
    source: int
    target: int
    label: str

class KnowledgeGraph(BaseModel):
    nodes: List[Node] = Field(..., default_factory=list)
    edges: List[Edge] = Field(..., default_factory=list)
```

----------------------------------------

TITLE: Executing Workflow Remotely with UnionRemote
DESCRIPTION: This code snippet initializes a `UnionRemote` object to execute a workflow on a remote Union cluster. It then executes the `build_indices_wf` workflow with a specific input (`local=False`) and prints the URL of the execution. This allows running the workflow on a remote cluster rather than locally.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
from union.remote import UnionRemote

remote = UnionRemote(default_project="default", default_domain="development")
```

LANGUAGE: python
CODE:
```
indices_execution = remote.execute(build_indices_wf, inputs={"local": False})
print(indices_execution.execution_url)
```

----------------------------------------

TITLE: Setting Up Together AI Client
DESCRIPTION: Initializes the Together AI client using API keys for authentication. This client will be used to upload files, create fine-tuning jobs, and monitor progress.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
## Setup Together AI client
from together import Together
import os
import json

TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
WANDB_API_KEY = os.getenv("WANDB_API_KEY") # needed for logging fine-tuning to wandb


client = Together(api_key=TOGETHER_API_KEY)
```

----------------------------------------

TITLE: Converting Preference Data to SFT Format for Two-Stage Fine-tuning
DESCRIPTION: Functions to convert DPO preference data format to SFT (Supervised Fine-Tuning) format. This preprocessing step enables a two-stage approach (SFT followed by DPO) which typically yields better results than applying DPO directly to a base model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
# Convert the preference dataset to SFT format
def convert_preference_to_sft_format(data):
    """
    Convert Preference data format to SFT format.
    
    Takes input messages and preferred output and formats them into a chat format
    with appropriate role assignments.
    """
    messages = []
    for msg in data["input"]["messages"]:
        messages.append(msg)
    messages.extend(data["preferred_output"])
    
    return {"messages": messages}

def process_preference_to_sft(input_data, output_path, split="train"):
    """
    Process the preference dataset and convert it to SFT format.
    
    Args:
        input_data: Dictionary containing train and validation preference data
        output_path: Path to save the output SFT jsonl file
        split: Dataset split to process ("train" or "validation")
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    line_count = 0
    with open(output_path, 'w') as outfile:
        for example in input_data[split]:
            try:
                sft_format = convert_preference_to_sft_format(example)
                outfile.write(json.dumps(sft_format) + '\n')
                line_count += 1
                if line_count % 2000 == 0 and split == "train":
                    print(f"Processed {line_count} examples")
            except Exception as e:
                print(f"Error processing example {line_count + 1}: {str(e)}")
    
    print(f"Processed {line_count} examples for {split}. Output saved to {output_path}")
    return line_count

# Process the training dataset
sft_train_output_path = "data/helpsteer2_sft_training.jsonl"
sft_train_count = process_preference_to_sft(converted_dataset, sft_train_output_path, "train")

# Process the validation dataset
sft_validation_output_path = "data/helpsteer2_sft_validation.jsonl"
sft_validation_count = process_preference_to_sft(converted_dataset, sft_validation_output_path, "validation")

# Display a sample SFT example from training set
with open(sft_train_output_path, 'r') as f:
    sample_sft = json.loads(f.readline().strip())
    
print("\nSample SFT example:")
print(json.dumps(sample_sft, indent=2))

# Compare dataset sizes
print(f"\nPreference dataset sizes:")
print(f"  Training: {len(converted_dataset['train'])} examples")
print(f"  Validation: {len(converted_dataset['validation'])} examples")

print(f"SFT dataset sizes:")
print(f"  Training: {sft_train_count} examples")
print(f"  Validation: {sft_validation_count} examples")
```

----------------------------------------

TITLE: Generating Movie Embeddings
DESCRIPTION: Calls the generate_embeddings function to create vector embeddings for all prepared movie texts using the BAAI/bge-base-en-v1.5 model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
embeddings = generate_embeddings(to_embed, 'BAAI/bge-base-en-v1.5')
```

----------------------------------------

TITLE: Displaying Worker Results for Each Subtask
DESCRIPTION: Prints the results from each worker LLM for their respective subtasks, showing how different aspects of the main task were handled.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Subtask_Agent_Workflow.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
for task_info, response in zip(tasks, worker_resp):
    print(f"\n=== WORKER RESULT ({task_info['type']}) ===\n{response}\n")
```

----------------------------------------

TITLE: Generating Query Embedding for Search
DESCRIPTION: Creates an embedding for a sample query about 'skip-level meetings' that will be used for semantic search in the vector space.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_22

LANGUAGE: python
CODE:
```
# Generate the vector embeddings for the query
query = "What are 'skip-level' meetings?"

query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]
```

----------------------------------------

TITLE: Importing Together API and Setting API Key
DESCRIPTION: Imports the Together library and sets up the API key, either from environment variables or manual input.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import together, os

# Paste in your Together AI API Key or load it
TOGETHER_API_KEY = os.environ.get("TOGETHER_API_KEY")
```

----------------------------------------

TITLE: Verifying Response Collection Completeness
DESCRIPTION: Performs a simple validation check to ensure all expected responses from the parallel LLM calls have been collected.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# Check we get all responses
assert len(results) == len(reference_models)
```

----------------------------------------

TITLE: Retrieving Top 5 Chunks by Similarity
DESCRIPTION: Gets the actual text content of the top 5 most similar chunks based on similarity scores.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
top_5_chunks = [chunks[index] for index in indices][:5]

top_5_chunks
```

----------------------------------------

TITLE: Printing Reranked Results in Python
DESCRIPTION: This snippet iterates through the results returned by the rerank model and prints the documents alongside their relevance scores to evaluate improvements in the search results.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_40

LANGUAGE: python
CODE:
```
for result in response.results:
    print(f"Document: {hybrid_top_k_docs[result.index]}")
    print(f"Relevance Score: {result.relevance_score}\n\n")
```

----------------------------------------

TITLE: Converting Extracted Text to Structured JSON
DESCRIPTION: Uses Llama 3.1 70B with JSON mode to convert the extracted text information into a structured JSON format conforming to the Receipt schema defined earlier.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
extract = client.chat.completions.create(
        messages=[
            {
                "role": "system",
                "content": "The following is a detailed description of all the items, prices and quantities on a receipt. Extract out information. Only answer in JSON.",
            },
            {
                "role": "user",
                "content": info,
            },
        ],
        model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        response_format={
            "type": "json_object",
            "schema": Receipt.model_json_schema(),
        },
    )
```

----------------------------------------

TITLE: Generating Movie Embeddings
DESCRIPTION: Generates embeddings for the movie texts using the BGE base English model. These embeddings capture the semantic meaning of each movie's details.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
# Use bge-base-en-v1.5 model to generate embeddings
embeddings = generate_embeddings(to_embed, 'BAAI/bge-base-en-v1.5')
```

----------------------------------------

TITLE: Evaluating Deployed Model - Python
DESCRIPTION: This snippet executes a loop to evaluate the performance of the fine-tuned model on the first 25 task items by calculating scores and length differences between expected and actual completions. The results are averaged and printed at the end.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_23

LANGUAGE: python
CODE:
```
scores = []
length_differences = []

for item in tqdm(task_items[:25]):
    query = item['prompt']

    # We have deployed the finetuned model here to evaluate it
    result = llm_call(query, model="thepowerfuldeez/Meta-Llama-3.1-8B-32k-Instruct-Reference-long-context-finetune-ce1f61d6-afb7623b")

    score = ratio(item['completion'], result)
    length_differences.append(abs(len(item['completion'].split()) - len(result.split())))
    scores.append(score)

print(np.mean(scores), np.mean(length_differences))
```

----------------------------------------

TITLE: Comparing Multiple Movie Embedding Vectors
DESCRIPTION: Visualizes and compares embedding vectors for the first three movies in the dataset as stacked barcode-like heatmaps.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
import matplotlib.pyplot as plt
import numpy as np

# Select three movie embeddings
data1 = embeddings[0]
data2 = embeddings[1]
data3 = embeddings[2]

# Reshape each embedding into a 2D array
data1_2d = np.reshape(data1, (1, 768))
data2_2d = np.reshape(data2, (1, 768))
data3_2d = np.reshape(data3, (1, 768))

# Create a figure with three subplots
fig, axs = plt.subplots(3, 1, figsize=(15, 3))

# Plot each embedding as a heatmap
axs[0].imshow(data1_2d, cmap='binary', interpolation='nearest', aspect='auto')
axs[1].imshow(data2_2d, cmap='binary', interpolation='nearest', aspect='auto')
axs[2].imshow(data3_2d, cmap='binary', interpolation='nearest', aspect='auto')

# Set labels and titles
axs[0].set_ylabel(f"{movies_data[0]['title']}")
axs[1].set_ylabel(f"{movies_data[1]['title']}")
axs[2].set_ylabel(f"{movies_data[2]['title']}")
axs[2].set_xlabel('Vector Dimension')

# Remove ticks
for ax in axs:
    ax.set_xticks([])
    ax.set_yticks([])

# Layout so plots do not overlap
fig.tight_layout()

# Show the plot
plt.show()
```

----------------------------------------

TITLE: Generating and Displaying Document Chunks
DESCRIPTION: Creates chunks from the essay with a size of 250 characters and 30 character overlap, then prints each chunk with its index for review.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
chunks = create_chunks(pg_essay, chunk_size=250, overlap=30)

for i, chunk in enumerate(chunks):
    print(f"Chunk {i + 1}: {chunk}")
```

----------------------------------------

TITLE: Extracting Long Documents (64k-128k Tokens)
DESCRIPTION: Filters the dataset to extract 2000 document examples with token counts between 64,000 and 128,000 for long context testing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# Extract 2000 examples of documents with 64k to 128k tokens

long_documents_128k = []
for sample in tqdm(ds_iterator.filter(lambda x: x['token_count'] > 64000 and x['token_count'] < 128000)):
    # From 64k tokens to 128k tokens
    if (len(long_documents_128k) < 2000):
        document = sample['text']
        long_documents_128k.append(document)
    else:
        break
```

----------------------------------------

TITLE: Creating Text Chunks with Overlap
DESCRIPTION: Defines a function to split the document into overlapping chunks of specified size. This chunking method creates fixed-size segments with a defined overlap between consecutive chunks.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# We can get away with naive fixed sized chunking as the context generation will add meaning to these chunks

def create_chunks(document, chunk_size=300, overlap=50):
    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]
```

----------------------------------------

TITLE: Displaying Structured JSON Output
DESCRIPTION: Parses and displays the final structured JSON output containing the receipt information with formatted indentation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
output = json.loads(extract.choices[0].message.content)
print(json.dumps(output, indent=2))
```

----------------------------------------

TITLE: Evaluating Model Performance on Multiple Tasks
DESCRIPTION: Runs the model on 25 task examples, calculating similarity scores and length differences to evaluate overall performance on the repetition task.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_18

LANGUAGE: python
CODE:
```
scores = []
length_differences = []
for item in tqdm(task_items[:25]):
    query = item['prompt']
    result = llm_call(query)
    score = ratio(item['completion'], result)
    length_differences.append(abs(len(item['completion'].split()) - len(result.split())))
    scores.append(score)
print(np.mean(scores), np.mean(length_differences))
```

----------------------------------------

TITLE: Retrieving Top 10 Most Relevant Movie Titles
DESCRIPTION: Extracts and displays the titles of the top 10 movies most semantically similar to the user query based on the sorted similarity scores.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_15

LANGUAGE: python
CODE:
```
top_10_sorted_titles = [movies_data[index]['title'] for index in indices[0]][:10]

top_10_sorted_titles
```

----------------------------------------

TITLE: Visualizing Performance Improvements with Seaborn in Python
DESCRIPTION: This code creates a distribution plot of the score differences between fine-tuned and baseline models using Seaborn. It visualizes how fine-tuning impacts model performance across the test dataset.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
sns.displot(score_differences)
plt.xlabel("GovReport fine-tuned - GovReport baseline")
```

----------------------------------------

TITLE: Displaying the Final Aggregated Response
DESCRIPTION: Prints the final answer generated by the aggregator model after analyzing all of the intermediate responses from the parallel workflow.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
print(f"Final Answer: {answer}\n")
```

----------------------------------------

TITLE: Parsing Main Page for Document URLs in Python
DESCRIPTION: Actor task that parses the main page of Paul Graham's essays to extract document titles and URLs using BeautifulSoup.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
@actor.task
def parse_main_page(
    base_url: str, articles_url: str, local: bool = False
) -> list[Document]:
    from bs4 import BeautifulSoup

    assert base_url.endswith("/"), f"Base URL must end with a slash: {base_url}"
    response = requests.get(urljoin(base_url, articles_url))
    soup = BeautifulSoup(response.text, "html.parser")

    td_cells = soup.select("table > tr > td > table > tr > td")
    documents = []

    idx = 0
    for td in td_cells:
        img = td.find("img")
        if img and int(img.get("width", 0)) <= 15 and int(img.get("height", 0)) <= 15:
            a_tag = td.find("font").find("a") if td.find("font") else None
            if a_tag:
                documents.append(
                    Document(
                        idx=idx, title=a_tag.text, url=urljoin(base_url, a_tag["href"])
                    )
                )
                idx += 1

    if local:
        return documents[:3]

    return documents
```

----------------------------------------

TITLE: Loading Pre-processed Task Items
DESCRIPTION: Loads the pre-processed repetition task prompts and completions from a JSON file for evaluation and fine-tuning.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
# Load the task items from provided JSON file

task_items = orjson.loads(Path("task_items.json").read_bytes())
task_items = task_items['task1']
```

----------------------------------------

TITLE: Counting Tokens in the Generated Summary
DESCRIPTION: Counts the number of tokens in the generated summary using the cl100k_base encoding to compare with the original document's token count.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
num_tokens(summary, 'cl100k_base')
```

----------------------------------------

TITLE: Evaluating the Optimized Agent Performance
DESCRIPTION: Measures the performance improvement of the optimized agent compared to the baseline using the same evaluation metric.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
evaluate(optimized_react)
```

----------------------------------------

TITLE: Executing a Single Tavily Search Query with Python
DESCRIPTION: This snippet executes a single search operation using the first query from a generated list. It demonstrates how to call the tavily_search function and process the returned results for review and analysis.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
search_results=await tavily_search(initial_queries[0], tavily_client, prompts, together_client, summary_model)
```

----------------------------------------

TITLE: Performing Image-to-Image Search Using JinaCLIP in Python
DESCRIPTION: Demonstrates image-to-image search by using a new image as a query to retrieve the most similar image from the dataset using the retrieve_image function.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
# Search the internet for a new image
new_image = search_images('cute pet dog running', max_images=1)[0]

display.Image(url=new_image, width=500)

# Use the image above as a query to retrieve the most similar image from our dataset of 12 images
image_2_image = links[retrieve_image(query=new_image, query_type='image', index=image_embeddings)]

display.Image(url=image_2_image, width=500)
```

----------------------------------------

TITLE: Initializing the Together AI Client
DESCRIPTION: Sets up the Together AI client using an API key stored in environment variables to access the Together AI services.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import os
from together import Together

# Paste in your Together AI API Key or load it

client = Together(api_key = os.environ.get("TOGETHER_API_KEY"))
```

----------------------------------------

TITLE: Validating JSONL File Format for Fine-tuning
DESCRIPTION: Checks that the prepared JSONL file meets Together AI's format requirements for fine-tuning. The function verifies that each line is a valid JSON object with the expected structure.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
## We're going to check to see that the file is in the right format before we finetune
from together.utils import check_file

sft_report = check_file("coqa_prepared_train.jsonl")
print(json.dumps(sft_report, indent=2))

assert sft_report["is_check_passed"] == True
```

----------------------------------------

TITLE: Viewing a HoVer Dataset Example
DESCRIPTION: Displays an example from the training set showing a claim and the correct Wikipedia pages that must be retrieved to verify it.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
# View an example
example = trainset[0]

print("Claim:", example.claim)
print("Correct pages that must be retrieved:", example.titles)
```

----------------------------------------

TITLE: Generating Query Embedding for Vector Search
DESCRIPTION: Creates an embedding for a specific query about floating point operations mentioned in the bill.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
# Generate the vector embeddings for the query
query = "what is the maximum allowable floating point operation per second this bill allows?"

query_embedding = generate_embeddings([query], 'BAAI/bge-large-en-v1.5')[0]
```

----------------------------------------

TITLE: Verifying Embedding Dimensions
DESCRIPTION: Checks the dimension of the first embedding vector to ensure it has the expected 768 dimensions.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Semantic_Search.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
# Check the shape of each vector embedding to make sure it is 768 dimensions
len(embeddings[0])
```

----------------------------------------

TITLE: Extracting Response Content from LLM Output in Python
DESCRIPTION: This line retrieves the content of the message from the LLM's response, which will be the final answer to the user's query based on the processed information.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_43

LANGUAGE: python
CODE:
```
response.choices[0].message.content
```

----------------------------------------

TITLE: Initializing Actor Environment and Document Model in Python
DESCRIPTION: Sets up the Union Actor environment with container configuration, dependencies, and secrets. Defines a Pydantic Document model for storing document metadata.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/contextual_rag_on_union/Contextual_RAG_on_Union.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import os
from pathlib import Path
from typing import Annotated, Optional
from urllib.parse import urljoin

import flytekit as fl
import numpy as np
import requests
import union
from flytekit.core.artifact import Artifact
from flytekit.exceptions.base import FlyteRecoverableException
from flytekit.types.directory import FlyteDirectory
from flytekit.types.file import FlyteFile
from pydantic import BaseModel
from union.actor import ActorEnvironment

actor = ActorEnvironment(
    name="contextual-rag",
    replica_count=10,
    ttl_seconds=120,
    container_image=union.ImageSpec(
        name="contextual-rag",
        packages=[
            "together==1.3.10",
            "beautifulsoup4==4.12.3",
            "bm25s==0.2.5",
            "pydantic>2",
            "pymilvus>=2.5.4",
            "union>=0.1.139",
            "flytekit>=1.15.0b5",
        ],
    ),
    secret_requests=[
        fl.Secret(
            key="together-api-key", 
            env_var="TOGETHER_API_KEY",
            mount_requirement=union.Secret.MountType.ENV_VAR,
        ),
        fl.Secret(
            key="milvus-uri",
            env_var="MILVUS_URI",
            mount_requirement=union.Secret.MountType.ENV_VAR,
        ),
        fl.Secret(
            key="milvus-token",
            env_var="MILVUS_TOKEN",
            mount_requirement=union.Secret.MountType.ENV_VAR,
        )
    ],
)

class Document(BaseModel):
    idx: int
    title: str
    url: str
    content: Optional[str] = None
    chunks: Optional[list[str]] = None
    prompts: Optional[list[str]] = None
    contextual_chunks: Optional[list[str]] = None
    tokens: Optional[list[list[int]]] = None
```

----------------------------------------

TITLE: Testing Wikipedia Search Function
DESCRIPTION: Tests the search function with a query about Toronto restaurants to retrieve relevant Wikipedia articles.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
search(query='toronto restaurants', k = 3)
```

----------------------------------------

TITLE: Generating NBA Playoffs Knowledge Graph Example
DESCRIPTION: Uses the previously defined function to generate a knowledge graph about the 2019 NBA playoffs, showing how the LLM structures information as nodes and edges.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# Lets use the function above to generate a knowledge graph about the 2019 NBA playoffs
# For context in the 2019 NBA playoffs, the Toronto Raptors won the NBA championship facing the Golden State Warriors.

graph = generate_graph("NBA 2019 playoffs")
```

----------------------------------------

TITLE: Loading CoQA Dataset for Conversational Finetuning
DESCRIPTION: Loads the CoQA dataset from the Hugging Face dataset repository. This dataset contains 127,000+ questions with answers from 8000+ conversations designed for Conversational Question Answering systems.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
from datasets import load_dataset

coqa_dataset = load_dataset("stanfordnlp/coqa")
```

----------------------------------------

TITLE: Generating Conditional Image Using Retrieved Image in Python
DESCRIPTION: Uses the generate_image function to create a holiday cartoon version of a retrieved image using the FLUX model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
generated_image = generate_image("Create a cute holiday cartoon version of this image.", retrieved_image = retrieved_image)
```

----------------------------------------

TITLE: Testing LLM Functionality with Simple Prompt
DESCRIPTION: Tests the configured Llama 8B model with a simple prompt to ensure it's working correctly.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/DSPy/DSPy_Agents.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
llama8b("Say this is a test!", temperature=0.7)  # => ['This is a test!']
#llama3b(messages=[{"role": "user", "content": "Say this is a test!"}])  # => ['This is a test!']
```

----------------------------------------

TITLE: Setting Up Together API Client and LLM Call Function
DESCRIPTION: Initializes the Together API client using environment variables and defines a helper function for making LLM API calls with consistent parameters.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Initialize the Together client and setup LLM calling function

TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
WANDB_API_KEY = os.getenv("WANDB_API_KEY") # If you'd like to view fine-tuning results on W&B

client = Together(api_key = TOGETHER_API_KEY)

def llm_call(query, model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"):
    response = client.chat.completions.create(
        model=model,
        messages=[
          {"role": "system", "content": "You are a helpful chatbot."},
          {"role": "user", "content": query},
        ],
        temperature=1.0,
        seed=42,
    )
    result = response.choices[0].message.content
    return result
```

----------------------------------------

TITLE: Library Imports and API Setup
DESCRIPTION: Importing required libraries and initializing the Together API client with authentication.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import json
import asyncio
import together
from together import Together

from typing import Any, Optional, Dict, List, Literal
from pydantic import Field, BaseModel, ValidationError

TOGETHER_API_KEY = "-- TOGETHER API KEY --"

client = Together(api_key= TOGETHER_API_KEY)
```

----------------------------------------

TITLE: Calculating Document Token Count
DESCRIPTION: Counts the number of tokens in the full essay using the cl100k_base encoding, which is compatible with many LLMs.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
# We'll assume we're using the "cl100k_base" encoding

num_tokens_from_string(pg_essay, "cl100k_base")
```

----------------------------------------

TITLE: Executing the Flight Booking Application in Python
DESCRIPTION: A simple code snippet demonstrating how to run the application using asyncio. This executes the main function which starts the booking process for a flight from SFO to ANC.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_9

LANGUAGE: python
CODE:
```
import asyncio

await main()
```

----------------------------------------

TITLE: Importing Libraries and Setting Up API Key
DESCRIPTION: Imports the necessary libraries for working with the Together AI API and sets up the API key from environment variables.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Knowledge_Graphs_with_Structured_Outputs.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
from together import Together
import together
import os, json

# Paste in your Together AI API Key or load it
TOGETHER_API_KEY = os.environ.get("TOGETHER_API_KEY")
```

----------------------------------------

TITLE: Configuring User Prompt for Tool-Based Task
DESCRIPTION: Creates a message prompt instructing the LLM to find and summarize tweets from a specific Twitter account. This demonstrates how to structure a request that will require tool use.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# User message to the LLM
messages = [
  {
    "role": "user",
    "content": "Find the last three messages from the account named @togethercompute on X/Twitter and summarize them in one sentence. Make it funny!",
  }
]
```

----------------------------------------

TITLE: Loading CoQA Dataset for Fine-tuning
DESCRIPTION: Loads the Stanford CoQA (Conversational Question Answering) dataset from Hugging Face Hub using the datasets library. This dataset contains stories with related questions and answers.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
from datasets import load_dataset

coqa_dataset = load_dataset("stanfordnlp/coqa")
```

----------------------------------------

TITLE: Generating User Notification with LLM
DESCRIPTION: This code snippet demonstrates how to use an LLM to generate a user-friendly message indicating that the image is ready to be viewed. It combines the original image request with the tool run output and sends it back to the LLM for message generation. The final LLM response containing the image URL is then printed to the console.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_14

LANGUAGE: python
CODE:
```
image_messages = generate_image_request + tool_run

last_response = client.chat.completions.create(
  model=MODEL,
  messages=image_messages,
  tools=tools,
)

print('LLM RESPONSE:', last_response.choices[0].message.content)
```

----------------------------------------

TITLE: Saving Transformed Dataset to JSONL File
DESCRIPTION: Exports the transformed dataset to a JSONL file where each line is a valid JSON object representing a conversation. This format is required for uploading to Together AI for fine-tuning.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Finetuning_Guide.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
train_messages.to_json("coqa_prepared_train.jsonl")
```

----------------------------------------

TITLE: Importing Required Packages for Together.AI and Toolhouse
DESCRIPTION: Imports the necessary Python libraries for working with the OpenAI SDK (compatible with Together's API) and Toolhouse SDK for accessing pre-built tools.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
# Import packages
try:
  import os
  from openai import OpenAI
  from toolhouse import Toolhouse
  print("Packages installed")
except:
  print("Packages not installed")
```

----------------------------------------

TITLE: Calculating Prompt Token Count
DESCRIPTION: Counts the number of tokens in the first prompt to estimate the input size for each LLM call.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_12

LANGUAGE: python
CODE:
```
num_tokens_from_string(prompts[0], "cl100k_base")
```

----------------------------------------

TITLE: Executing BM25 Retrieval Function in Python
DESCRIPTION: This line executes the bm25 retrieval function with a specific query to find the top k relevant documents according to the query's content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_33

LANGUAGE: python
CODE:
```
bm25_retreival(query = "What are 'skip-level' meetings?", k = 5, bm25_index = retriever)
```

----------------------------------------

TITLE: Creating Text Chunks for Document Processing
DESCRIPTION: A function to divide the document into overlapping chunks of fixed size for easier processing and retrieval.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# We can get away with naive fixed sized chunking as the context generation will add meaning to these chunks

def create_chunks(document, chunk_size=300, overlap=50):
    return [document[i : i + chunk_size] for i in range(0, len(document), chunk_size - overlap)]
```

----------------------------------------

TITLE: Implementing Model Answer Generation for CoQA Evaluation
DESCRIPTION: Defines a function that generates answers from the model for evaluation. For each story in the validation set, it sends questions to the model and collects the generated answers for later comparison with ground truth.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
# This function is used to generate model answers on the CoQA validation set from the untuned reference and fine-tuned models

def get_model_answers(model_name):
    """
    Generate model answers for a given model name using a dataset of questions and answers.
    Args:
        model_name (str): The name of the model to use for generating answers.
    Returns:
        list: A list of lists, where each inner list contains the answers generated by the model for the corresponding set of questions in the dataset.
    The function performs the following steps:
    1. Initializes an empty list to store the model answers.
    2. Defines an inner function `get_answers` that takes a data dictionary and generates answers for the questions in the data.
    3. Uses a thread pool to parallelize the process of generating answers for each entry in the validation dataset.
    4. Appends the generated answers to the `model_answers` list.
    5. Returns the `model_answers` list.
    Note:
        - The `system_prompt` and `client` variables are assumed to be defined elsewhere in the code.
        - The `coqa_dataset` variable is assumed to contain the dataset with a "validation" key.
    """

    model_answers = []

    def get_answers(data):
        answers = []
        messages = [
            {
                "role": "system",
                "content": system_prompt.format(data["story"]),
            }
        ]
        for q, true_answer in zip(data["questions"], data["answers"]["input_text"]):
            messages.append(
                {
                    "role": "user",
                    "content": q
                }
            )
            chat_completion = client.chat.completions.create(
                messages=messages,
                model=model_name,
                max_tokens=64,
            )
            answer = chat_completion.choices[0].message.content
            answers.append(answer)
        return answers


    with ThreadPool(8) as pool:
        for answers in tqdm(pool.imap(get_answers, coqa_dataset["validation"]), total=len(coqa_dataset["validation"])):
            model_answers.append(answers)

    return model_answers
```

----------------------------------------

TITLE: Retrieving Top 25 Similar Movies
DESCRIPTION: Extracts the titles of the 25 movies most similar to the query based on the sorted similarity scores. These will be the candidates for reranking.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_13

LANGUAGE: python
CODE:
```
# Get the top 25 movie titles that are most similar to the query - these will be passed to the reranker
top_25_sorted_titles = [movies_data[index]['title'] for index in indices[0]][:25]

top_25_sorted_titles
```

----------------------------------------

TITLE: Applying Chunking to Document and Inspecting Results
DESCRIPTION: Creates chunks from the document text with specified size and overlap parameters, then displays the first few chunks for inspection.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
chunks = create_chunks(text, chunk_size=350, overlap=40)

for i, chunk in enumerate(chunks[:3]):
    print(f"Chunk {i + 1}: {chunk}")
```

----------------------------------------

TITLE: LLM Helper Functions - API Interaction
DESCRIPTION: Implements helper functions for making LLM API calls with support for both regular and JSON-structured responses
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Conditional_Router_Agent_Workflow.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
def run_llm(user_prompt : str, model : str, system_prompt : Optional[str] = None):
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    
    messages.append({"role": "user", "content": user_prompt})
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.7,
        max_tokens=4000,        
    )

    return response.choices[0].message.content

def JSON_llm(user_prompt : str, schema : BaseModel, system_prompt : Optional[str] = None):
    try:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        messages.append({"role": "user", "content": user_prompt})
        
        extract = client.chat.completions.create(
            messages=messages,
            model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            response_format={
                "type": "json_object",
                "schema": schema.model_json_schema(),
            },
        )
        
        response = json.loads(extract.choices[0].message.content)
        return response
        
    except ValidationError as e:
        raise ValueError(f"Schema validation failed: {str(e)}")
```

----------------------------------------

TITLE: Printing Extracted Text Information
DESCRIPTION: Prints the raw text information extracted from the receipt image by the vision model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Structured_Text_Extraction_from_Images.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
print(info)
```

----------------------------------------

TITLE: Setting Up Environment and Initializing Together API Client in Python
DESCRIPTION: This code sets up the environment by importing required libraries, initializing the Together API client, and defining a function for making LLM calls. It requires the TOGETHER_API_KEY environment variable to be set.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_LongContext_Finetuning.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
from pathlib import Path
from tqdm.auto import tqdm
from together import Together

import orjson
import numpy as np
import os
import pandas as pd

from evaluate import load

TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
#WANDB_API_KEY = os.getenv("WANDB_API_KEY")

client = Together(api_key = TOGETHER_API_KEY)

def llm_call(query, model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"):
    response = client.chat.completions.create(
        model=model,
        messages=[
          {"role": "system", "content": "You are a helpful chatbot."},
          {"role": "user", "content": query},
        ],
        temperature=1.0,
        seed=42,
        max_tokens=1200,
    )
    result = response.choices[0].message.content
    return result
```

----------------------------------------

TITLE: Creating Movie Text Embeddings
DESCRIPTION: Processes movie data by concatenating title, overview, and tagline for embedding generation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
# Concatenate the title, overview, and tagline of each movie
to_embed = []
for movie in movies_data[:1000]:
    text = ''
    for field in ['title', 'overview', 'tagline']:
        value = movie.get(field, '')
        text += str(value) + ' '
    to_embed.append(text.strip())

to_embed[:10]
```

----------------------------------------

TITLE: Generating Realistic Sketch with Multiple LoRAs
DESCRIPTION: Shows how to combine a sketch LoRA with a realism-enhancing LoRA to create a realistic sketch. It demonstrates the use of trigger words and different scale values for each LoRA.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Flux_LoRA_Inference.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
prompt = "an baby panda eating bamboo illustration in the style of SMPL"

generated_image = generate_image(prompt, 
                                lora1="https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-add-details",
                                scale1=0.9,
                                lora2="https://huggingface.co/dvyio/flux-lora-simple-illustration",
                                scale2=1.25
                                )

Image(url=generated_image, width=512, height=384)
```

----------------------------------------

TITLE: Generating Irrelevant Summary using Python
DESCRIPTION: This snippet generates an irrelevant summary using a predefined prompt and prints the result. It requires the 'summarize' function and the 'SUMMARIZATION_PROMPT_IRRELEVANT'.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_15

LANGUAGE: Python
CODE:
```
"""
summary_irrelevant = summarize(text, SUMMARIZATION_PROMPT_IRRELEVANT)
print(summary_irrelevant)
"""
"
```

----------------------------------------

TITLE: Downloading Movie Dataset
DESCRIPTION: Downloads and sets up the movie dataset JSON file for RAG implementation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Let's get the movies dataset
!wget https://raw.githubusercontent.com/togethercomputer/together-cookbook/refs/heads/main/datasets/movies.json
!mkdir datasets
!mv movies.json datasets/movies.json
```

----------------------------------------

TITLE: Executing Vector Retrieval Function in Python
DESCRIPTION: This line calls the vector retrieval function with a specific query and context embeddings, aiming to retrieve the top k items based on similarity to the query.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_28

LANGUAGE: python
CODE:
```
vector_retreival(query = "What are 'skip-level' meetings?", top_k = 5, vector_index = contextual_embeddings)
```

----------------------------------------

TITLE: Initializing API Clients for Research Tools in Python
DESCRIPTION: Sets up the API clients for Together (LLM provider) and Tavily (search provider) with placeholders for API keys, which are essential for the research pipeline to access external services.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Initialize Clients
# ------------------
# Initialize the Together and Tavily clients with your API keys
together_client = AsyncTogether(api_key="--your--together-api-key--")
tavily_client = AsyncTavilyClient(api_key="--your--tavily-api-key--")
```

----------------------------------------

TITLE: Analyzing Tool Call Details
DESCRIPTION: Examines the details of the tool calls made by the LLM, showing the ID, type, and function arguments for each tool that was selected for use in fulfilling the user's request.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
tools_called = response.choices[0].message.tool_calls
for tool_called in tools_called:
    print(f"ID: {tool_called.id}")
    print(f"Type: {tool_called.type}")
    print(f"Function: {tool_called.function}")
    print('\n')
```

----------------------------------------

TITLE: Downloading and Parsing Legislative Document
DESCRIPTION: Fetches the text content of California's SB1047 bill from LegiScan using requests and BeautifulSoup for HTML parsing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/RAG_with_Reasoning_Models.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
import requests
from bs4 import BeautifulSoup

def get_legiscan_text(url):
    """
    Fetches and returns the text content from a given LegiScan URL.
    Args:
        url (str): The URL of the LegiScan page to fetch.
    Returns:
        str: The text content of the page.
    Raises:
        requests.exceptions.RequestException: If there is an issue with the HTTP request.
    """
    # Basic headers to mimic a browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    # Make the request
    response = requests.get(url, headers=headers)

    # Parse HTML
    soup = BeautifulSoup(response.text, 'html.parser')

    # Get text content
    content = soup.get_text()

    return content

url = "https://legiscan.com/CA/text/SB1047/id/2999979/California-2023-SB1047-Amended.html"
text = get_legiscan_text(url)
print(text[:1000])
```

----------------------------------------

TITLE: Scraping Essay Content from a Web Page
DESCRIPTION: Defines a function to scrape Paul Graham's essay from his website using BeautifulSoup. The function retrieves the HTML content, extracts the text, and cleans it for processing.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
# Let's download the essay from Paul Graham's website

import requests
from bs4 import BeautifulSoup

def scrape_pg_essay():

    url = 'https://paulgraham.com/foundermode.html'

    try:
        # Send GET request to the URL
        response = requests.get(url)
        response.raise_for_status()  # Raise an error for bad status codes

        # Parse the HTML content
        soup = BeautifulSoup(response.text, 'html.parser')

        # Paul Graham's essays typically have the main content in a font tag
        # You might need to adjust this selector based on the actual HTML structure
        content = soup.find('font')

        if content:
            # Extract and clean the text
            text = content.get_text()
            # Remove extra whitespace and normalize line breaks
            text = ' '.join(text.split())
            return text
        else:
            return "Could not find the main content of the essay."

    except requests.RequestException as e:
        return f"Error fetching the webpage: {e}"

# Scrape the essay
pg_essay = scrape_pg_essay()
```

----------------------------------------

TITLE: Iterative Research Function in Python
DESCRIPTION: This function conducts iterative research within a given budget to refine search results. It takes a research topic, initial results, a list of all queries used so far, a budget, maximum queries per iteration, Tavily and Together AI clients, models for planning, JSON parsing, and summarization, and a dictionary of prompt templates as input. It returns a tuple of the final search results and all queries used.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
async def conduct_iterative_research(topic: str, initial_results: SearchResults, all_queries: List[str],
                                  budget: int, max_queries: int, tavily_client: AsyncTavilyClient, together_client: AsyncTogether,
                                  planning_model: str, json_model: str, summary_model: str, prompts: dict) -> tuple[SearchResults, List[str]]:
    """
    Conduct iterative research within budget to refine results.

    Args:
        topic: The research topic
        initial_results: Results from initial search
        all_queries: List of all queries used so far
        budget: Maximum number of follow-up iterations
        max_queries: Maximum number of queries to use per iteration
        tavily_client: The Tavily client for web search
        together_client: The Together AI client for LLM operations
        planning_model: Model to use for evaluation
        json_model: Model to use for JSON parsing
        summary_model: Model to use for summarization   
        prompts: Dictionary of prompt templates

    Returns:
        Tuple of (final results, all queries used)
    """
    results = initial_results

    for _ in range(0, budget):
        # Evaluate if more research is needed using the independent function
        additional_queries = await evaluate_research_completeness(
            topic, results, all_queries, together_client, planning_model, json_model, prompts
        )

        # Exit if research is complete
        if not additional_queries:
            print("No need for additional research")
            break

        # Limit the number of queries if needed
        if max_queries > 0:
            additional_queries = additional_queries[:max_queries]
        print("================================================\n\n")
        print(f"Additional queries from evaluation parser: {additional_queries}\n\n")
        print("================================================\n\n")

        # Expand research with new queries
        new_results = await perform_search(
            additional_queries,
            tavily_client,
            prompts,
            together_client,
            summary_model
        )

        results = results + new_results
        all_queries.extend(additional_queries)

    return results, all_queries
```

----------------------------------------

TITLE: Fetching Top 5 Indices From Similarity Scores in Python
DESCRIPTION: This snippet retrieves the top 5 indices from the sorted similarity scores. It is crucial for identifying the most similar items to a given query.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_25

LANGUAGE: python
CODE:
```
top_5_indices = indices[0][:5]
top_5_indices
```

----------------------------------------

TITLE: Plotting Smoothed BERTScores Comparison in Python
DESCRIPTION: This snippet smooths a specific BERTScore component and plots it alongside a relevant summary's score. Required dependencies include Matplotlib and NumPy for data plotting and manipulation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_18

LANGUAGE: Python
CODE:
```
"""
smoothed_scores_irrelevant = rolling_window(scores_irrelevant[2], 3)

plt.plot(np.linspace(0, 1, len(smoothed_scores_irrelevant)), smoothed_scores_irrelevant, label = "Irrelevant Summary")
plt.plot(np.linspace(0, 1, len(smoothed_F1)), smoothed_F1, label = "Relevant Summary")
plt.xlabel('Position in the Document')
plt.ylabel('Score - higher is better')
plt.title('Summary Score')
plt.legend()
plt.show()
"""
"
```

----------------------------------------

TITLE: Displaying Essay Content Preview
DESCRIPTION: Prints the first 1000 characters of the scraped essay to verify the content was retrieved correctly.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
print(pg_essay[:1000])
```

----------------------------------------

TITLE: Creating a Dataset of Diverse Image Links in Python
DESCRIPTION: Generates a small dataset of 12 image links by searching for diverse topics using the custom search_images function.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# Lets create a small dataset of 12 images containing diverse topics
searches = 'forest', 'dog', 'strawberry field', 'family picture'

from time import sleep

links = []

for o in searches:
    links += search_images(o, max_images=3)
    sleep(1)
```

----------------------------------------

TITLE: Executing Research Topic Processing in Python
DESCRIPTION: Example code for running the research pipeline with a specific topic about AI in manufacturing. This demonstrates the practical application of the query generation system with the configured models and parameters.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Together_Open_Deep_Research_CookBook.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
research_topic="The impact of artificial intelligence on the manufacturing industry"

initial_queries = await generate_initial_queries(
    topic=research_topic,
    together_client=together_client,
    max_queries=max_queries,
    planning_model=planning_model,
    json_model=json_model,
    prompts=prompts
)
```

----------------------------------------

TITLE: Implementing Image Retrieval Function Using JinaCLIP in Python
DESCRIPTION: Defines a function to retrieve the most semantically relevant image based on a text or image query using the JinaCLIP model embeddings.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
import numpy as np

def retrieve_image(query, query_type, index):
    """
    Retrieve the index of the most similar image based on a query.
    Args:
        query (str or PIL.Image or str): The query input, which can be a text string,
                                         a PIL image, or a local filename.
        query_type (str): The type of the query, either 'text' or 'image'.
        index (int): The index of the image to be retrieved.
    Returns:
        int: The index of the most similar image based on the query.
    Raises:
        ValueError: If the query_type is not 'text' or 'image'.
    """

    if query_type == 'text':
        query_embedding = model.encode_text(query)
    elif query_type == 'image':
        query_embedding = model.encode_image(query) # Accepts PIL.image, local filenames, dataURI
    else:
        raise ValueError("query_type must be 'text' or 'image'")

    similarities = query_embedding @ index.T # We calculate the similaritry between the query embedding and all the image embeddings

    return np.argmax(similarities)
```

----------------------------------------

TITLE: Splitting Function Calling Dataset for Two-Step Training
DESCRIPTION: Splits the dataset into two parts: one for initial training on core categories and another for subsequent training on remaining categories.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/Continual_Finetuning.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
first_step_categories = [
    "API Call",
    "Information Extraction",
    'Industrial Software',
    'Utilities Software',
    'Robotic Process Automation (RPA)',
    'Financial Software',
    'Healthcare Software',
    'Algorithmic Trading',
]

first_step_train = function_calling["train"].filter(lambda x: x["category"] in first_step_categories)
second_step_train = function_calling["train"].filter(lambda x: x["category"] not in first_step_categories)
```

----------------------------------------

TITLE: Displaying Available Toolhouse Tools
DESCRIPTION: Lists all the tools available in the specified Toolhouse bundle, showing their names, types, and descriptions to help users understand what tools they can use in their application.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_4

LANGUAGE: python
CODE:
```
print('TOOLS AVAILABLE:')
for tool in th.get_tools(bundle="togethertoolhouse"):
    print(f"Name: {tool['function']['name']}")
    print(f"Type: {tool['type']}")
    print(f"Description: {tool['function']['description']}")
    print("--------")
```

----------------------------------------

TITLE: Generating Training Data - Python
DESCRIPTION: This snippet creates a JSONL file that excludes the first 25 task items for training purposes, saving the remaining items for model training using orjson. The generated file is 'task_train.jsonl'.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_20

LANGUAGE: python
CODE:
```
Path("task_train.jsonl").write_text("\n".join([orjson.dumps(item).decode("utf-8") for item in task_items[25:]]))
```

----------------------------------------

TITLE: Installing Required Libraries for Parallel Agent Workflow
DESCRIPTION: Installs the necessary Python libraries (pydantic and together) for implementing a parallel agent workflow.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Parallel_Agent_Workflow.ipynb#2025-04-19_snippet_0

LANGUAGE: python
CODE:
```
# Install libraries
!pip install -qU pydantic together
```

----------------------------------------

TITLE: Uploading Training File - Python
DESCRIPTION: This snippet uploads the training file 'task_train.jsonl' to the Together AI client, ensuring the upload completes with a check. The uploaded file's id is stored in 'task_file_id'.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/LongContext_Finetuning_RepetitionTask.ipynb#2025-04-19_snippet_21

LANGUAGE: python
CODE:
```
response = client.files.upload(file="task_train.jsonl", check=True)
task_file_id = response.id
```

----------------------------------------

TITLE: Customizing Gmail Tool with Preprocessors and Schema Processors
DESCRIPTION: This code customizes the email behavior using preprocessors and schema processors. It defines functions to modify the input before sending (changing recipient email) and to modify the tool schema (removing fields from LLM's view). It then creates a processed email tool with both processors.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Composio/Agents_Composio.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
# Preprocessor to override recipient email
def gmail_preprocessor(inputs: dict) -> dict:
    inputs["recipient_email"] = "my_email.dev"  # Change to an email you can access to test!
    return inputs

# Schema processor to hide recipient field from LLM
def gmail_schema_processor(schema: dict) -> dict:
    del schema["recipient_email"]
    return schema

# Create processed email tool with both processors
processed_send_email_tool = toolset.get_tools(
    actions=[Action.GMAIL_SEND_EMAIL],
    processors={
        "schema": {Action.GMAIL_SEND_EMAIL: gmail_schema_processor},
        "pre": {Action.GMAIL_SEND_EMAIL: gmail_preprocessor},
    },
    check_connected_accounts=True,
)
```

----------------------------------------

TITLE: Generating Thought Process with DeepSeek-R1 via Together API in Python
DESCRIPTION: This snippet uses the DeepSeek-R1 model to generate a thought process for a given question. It demonstrates how to use a stop token to control the generation.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
thought = client.chat.completions.create(
  model="deepseek-ai/DeepSeek-R1",
  messages=[{"role": "user", "content": question}],
  stop = ['</think>'] # Stop generation when </think> is encountered
)

print(thought.choices[0].message.content)
```

----------------------------------------

TITLE: Testing Context Generation on a Single Chunk
DESCRIPTION: Generates context for the first chunk and concatenates it with the original chunk to demonstrate the output format.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_16

LANGUAGE: python
CODE:
```
# Lets generate one context and concatenate it with its chunk to see what the output looks like
context_0 = generate_context(prompts[0])
print(context_0 + " \n\n" + chunks[0])
```

----------------------------------------

TITLE: Retrieving Base64 Encoded Image for Llama 3.2 90B Vision
DESCRIPTION: Retrieves the base64 encoded image of the most relevant page for the given query, to be used as input for the Llama 3.2 90B Vision model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/MultiModal_RAG_with_Nvidia_Investor_Slide_Deck.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
model.search(query, k=1)

returned_page = model.search(query, k=1)[0].base64
```

----------------------------------------

TITLE: Initializing Together AI Client
DESCRIPTION: Setup of Together AI client with API key configuration.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Text_RAG.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import together, os
from together import Together

# Paste in your Together AI API Key or load it
TOGETHER_API_KEY = os.environ.get("TOGETHER_API_KEY")
```

----------------------------------------

TITLE: Installing Together AI Library
DESCRIPTION: Installs the Together AI Python library using pip, which is required to access the Together AI API for embeddings and reranking.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Search_with_Reranking.ipynb#2025-04-19_snippet_0

LANGUAGE: python
CODE:
```
!pip install together
```

----------------------------------------

TITLE: Searching for Images Using Custom Function in Python
DESCRIPTION: Demonstrates the usage of the custom search_images function to retrieve image URLs based on a search query.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multimodal_Search_and_Conditional_Image_Generation.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# Our function will allow us to search the web for images
search_images('family picture', max_images=3)
```

----------------------------------------

TITLE: Calculating BERTScores for Irrelevant Summary in Python
DESCRIPTION: This snippet calculates the BERTScores for the irrelevant summary using 'chunked_BERTscores'. It assumes proper installation and setup of BERTScore dependencies.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_16

LANGUAGE: Python
CODE:
```
"""
scores_irrelevant = chunked_BERTscores(text, summary_irrelevant)
"""
"
```

----------------------------------------

TITLE: Generating Summary Response from Tool Data
DESCRIPTION: Uses the LLM to generate a funny summary of the Twitter messages retrieved by the tool. This demonstrates using tool-fetched data to generate creative content.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/third_party_integrations/Tool_use_with_Toolhouse.ipynb#2025-04-19_snippet_10

LANGUAGE: python
CODE:
```
summary_response = client.chat.completions.create(
  model=MODEL,
  messages=messages
)

print('LLM RESPONSE:')
print(summary_response.choices[0].message.content)
```

----------------------------------------

TITLE: Verifying Contextual Chunks Generation
DESCRIPTION: Confirms that the number of contextual chunks matches the number of original chunks, ensuring all chunks have been processed.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_18

LANGUAGE: python
CODE:
```
# We should have one contextual chunk for each original chunk
len(contextual_chunks), len(contextual_chunks) == len(chunks)
```

----------------------------------------

TITLE: Token Counting Function Using Tiktoken
DESCRIPTION: Defines a function to count the number of tokens in a given text string using a specified encoding, which is useful for ensuring text fits within LLM context windows.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_2

LANGUAGE: python
CODE:
```
# token counter function
import tiktoken

def num_tokens(string: str, encoding_name: str) -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens
```

----------------------------------------

TITLE: Examining Transformed CoQA Dataset
DESCRIPTION: Displays the transformed CoQA dataset in its new conversational format to verify the structure before saving it for finetuning.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_5

LANGUAGE: python
CODE:
```
train_messages
```

----------------------------------------

TITLE: Listing Files in Data Directory
DESCRIPTION: A simple shell command to list files in the data directory, executed using the Jupyter notebook magic command for shell execution.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb#2025-04-19_snippet_11

LANGUAGE: python
CODE:
```
!ls data
```

----------------------------------------

TITLE: Initializing Together AI Client for Finetuning
DESCRIPTION: Sets up the Together AI client with API credentials for model finetuning. This includes retrieving API keys from environment variables for both Together AI and Weights & Biases for tracking.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Multiturn_Conversation_Finetuning.ipynb#2025-04-19_snippet_7

LANGUAGE: python
CODE:
```
from together import Together
import os

TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
WANDB_API_KEY = os.getenv("WANDB_API_KEY")


client = Together(api_key=TOGETHER_API_KEY)
```

----------------------------------------

TITLE: Defining BM25 Retrieval Function in Python
DESCRIPTION: This function retrieves the top-k document indices based on the BM25 algorithm given a search query. It takes the query string, the number of documents to retrieve, and the BM25 index object, and returns the indices of the matched documents.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Open_Contextual_RAG.ipynb#2025-04-19_snippet_32

LANGUAGE: python
CODE:
```
def bm25_retreival(query: str, k : int, bm25_index) -> List[int]:
    """
    Retrieve the top-k document indices based on the BM25 algorithm for a given query.
    Args:
        query (str): The search query string.
        k (int): The number of top documents to retrieve.
        bm25_index: The BM25 index object used for retrieval.
    Returns:
        List[int]: A list of indices of the top-k documents that match the query.
    """

    results, scores = bm25_index.retrieve(bm25s.tokenize(query), k=k)

    return [contextual_chunks.index(doc) for doc in results[0]]
```

----------------------------------------

TITLE: Importing Libraries and Configuring LLM with Together API
DESCRIPTION: This code block imports necessary libraries and configures the language model using Together's API. It sets up the Llama-3.3-70B model for use with PydanticAI.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Agents/PydanticAI/PydanticAI_Agents.ipynb#2025-04-19_snippet_1

LANGUAGE: python
CODE:
```
import datetime, os
from dataclasses import dataclass
from typing import Literal

from pydantic import BaseModel, Field
from rich.prompt import Prompt

from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.messages import ModelMessage
from pydantic_ai.usage import Usage, UsageLimits

from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider

# Connect PydanticAI to LLMs on Together
llm = OpenAIModel('meta-llama/Llama-3.3-70B-Instruct-Turbo',
                  provider=OpenAIProvider(
                      base_url="https://api.together.xyz/v1",
                      api_key=os.environ.get("TOGETHER_API_KEY"),
                      ),
                   )
```

----------------------------------------

TITLE: Generating and Displaying the Summary
DESCRIPTION: Calls the summarize function to generate a summary of the legislative text using the previously defined prompt template and prints the result.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Summarization_Evaluation.ipynb#2025-04-19_snippet_6

LANGUAGE: python
CODE:
```
summary = summarize(text, SUMMARIZATION_PROMPT)
print(summary)
```

----------------------------------------

TITLE: Extracting Movie Genres and Titles
DESCRIPTION: Extracts the primary genre and title for each movie from the dataset for use in visualization.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Embedding_Visualization.ipynb#2025-04-19_snippet_8

LANGUAGE: python
CODE:
```
# Extract the genres and titles for each movie
genres = [movie['genres'].split()[0] for movie in movies_data]
titles = [movie['title'] for movie in movies_data]
set(genres)
```

----------------------------------------

TITLE: Defining Prompt Template for Thinking Augmented Generation in Python
DESCRIPTION: This code defines a prompt template that incorporates both the original question and the generated thought process. It's used to structure the input for the smaller model.
SOURCE: https://github.com/togethercomputer/together-cookbook/blob/main/Thinking_Augmented_Generation.ipynb#2025-04-19_snippet_3

LANGUAGE: python
CODE:
```
PROMPT_TEMPLATE = """
Question: {question}
Thought process: {thinking_tokens} </think>
Answer:
"""
```